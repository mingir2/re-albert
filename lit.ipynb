{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w7N9tY3ugYO"
      },
      "source": [
        "!pip install lit-nlp\n",
        "# Install transformers v2.11.0 for to address compatibility issue.\n",
        "!pip install transformers==2.11.0\n",
        "# Use the nighly version to prevent tensorflow-datsets causing error due to incorrect path.\n",
        "!pip uninstall tensorflow-datasets -y\n",
        "!pip install tfds-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPGu_YeQvU-x",
        "outputId": "1997054f-4cad-4e0b-ff8a-d07701165ea1"
      },
      "source": [
        "# Set up ngrok\n",
        "%%sh\n",
        "wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "\n",
        "./ngrok authtoken {NGROK_TOKEN}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaibNIgb2qdx",
        "outputId": "16428845-74c5-4231-9187-e2970bae5cb7"
      },
      "source": [
        "%%writefile glue_models/__init__.py\n",
        "\n",
        "# Lint as: python3\n",
        "\"\"\"Wrapper for fine-tuned HuggingFace models in LIT.\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, List, Iterable\n",
        "\n",
        "from absl import logging\n",
        "import attr\n",
        "from lit_nlp.api import model as lit_model\n",
        "from lit_nlp.api import types as lit_types\n",
        "from lit_nlp.lib import utils\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "\n",
        "JsonDict = lit_types.JsonDict\n",
        "Spec = lit_types.Spec\n",
        "\n",
        "\n",
        "def _from_pretrained(cls, *args, **kw):\n",
        "  \"\"\"Load a transformers model in TF2, with fallback to PyTorch weights.\"\"\"\n",
        "  try:\n",
        "    return cls.from_pretrained(*args, **kw)\n",
        "  except OSError as e:\n",
        "    logging.warning(\"Caught OSError loading model: %s\", e)\n",
        "    logging.warning(\n",
        "        \"Re-trying to convert from PyTorch checkpoint (from_pt=True)\")\n",
        "    return cls.from_pretrained(*args, from_pt=True, **kw)\n",
        "\n",
        "\n",
        "@attr.s(auto_attribs=True, kw_only=True)\n",
        "class GlueModelConfig(object):\n",
        "  \"\"\"Config options for a GlueModel.\"\"\"\n",
        "  # Preprocessing options\n",
        "  max_seq_length: int = 128\n",
        "  inference_batch_size: int = 32\n",
        "  # Input options\n",
        "  text_a_name: str = \"sentence1\"\n",
        "  text_b_name: Optional[str] = \"sentence2\"  # set to None for single-segment\n",
        "  label_name: str = \"label\"\n",
        "  # Output options\n",
        "  labels: Optional[List[str]] = None  # set to None for regression\n",
        "  null_label_idx: Optional[int] = None\n",
        "  compute_grads: bool = True  # if True, compute and return gradients.\n",
        "\n",
        "\n",
        "class GlueModel(lit_model.Model):\n",
        "  \"\"\"GLUE benchmark model, using Keras/TF2 and Huggingface Transformers.\n",
        "  This is a general-purpose classification or regression model. It works for\n",
        "  one- or two-segment input, and predicts either a multiclass label or\n",
        "  a regression score. See GlueModelConfig for available options.\n",
        "  This implements the LIT API for inference (e.g. input_spec(), output_spec(),\n",
        "  and predict()), but also provides a train() method to run fine-tuning.\n",
        "  This is a full-featured implementation, which includes embeddings, attention,\n",
        "  gradients, as well as support for the different input and output types above.\n",
        "  For a more minimal example, see ../simple_tf2_demo.py.\n",
        "  \"\"\"\n",
        "\n",
        "  @property\n",
        "  def is_regression(self) -> bool:\n",
        "    return self.config.labels is None\n",
        "\n",
        "  # TODO(lit-dev): upgrade version of huggingface so we can just pass\n",
        "  # output_hidden_states and output_attentions at inference time, rather than\n",
        "  # as part of the model config. Then we don't need a special for_training mode.\n",
        "  def __init__(self,\n",
        "               model_name_or_path=\"bert-base-uncased\",\n",
        "               for_training=False,\n",
        "               **config_kw):\n",
        "    self.config = GlueModelConfig(**config_kw)\n",
        "    self._load_model(model_name_or_path, for_training)\n",
        "\n",
        "  def _load_model(self, model_name_or_path, for_training):\n",
        "    \"\"\"Load model. Can be overridden for testing.\"\"\"\n",
        "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path)\n",
        "    model_config = transformers.AutoConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        num_labels=1 if self.is_regression else len(self.config.labels),\n",
        "        output_hidden_states=(not for_training),\n",
        "        output_attentions=(not for_training),\n",
        "    )\n",
        "    self.model = _from_pretrained(\n",
        "        transformers.TFAutoModelForSequenceClassification,\n",
        "        model_name_or_path,\n",
        "        config=model_config)\n",
        "\n",
        "  def _preprocess(self, inputs: Iterable[JsonDict]) -> Dict[str, tf.Tensor]:\n",
        "    segments = [\n",
        "        (ex[self.config.text_a_name],\n",
        "         ex[self.config.text_b_name] if self.config.text_b_name else None)\n",
        "        for ex in inputs\n",
        "    ]\n",
        "    encoded_input = self.tokenizer.batch_encode_plus(\n",
        "        segments,\n",
        "        return_tensors=\"tf\",\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.config.max_seq_length,\n",
        "        pad_to_max_length=True)\n",
        "    # Trim everything to the actual max length, to remove extra padding.\n",
        "    max_tokens = tf.reduce_max(\n",
        "        tf.reduce_sum(encoded_input[\"attention_mask\"], axis=1))\n",
        "    encoded_input = {k: v[:, :max_tokens] for k, v in encoded_input.items()}\n",
        "    return encoded_input\n",
        "\n",
        "  def _make_dataset(self, inputs: Iterable[JsonDict]) -> tf.data.Dataset:\n",
        "    \"\"\"Make a tf.data.Dataset from inputs in LIT format.\"\"\"\n",
        "    encoded_input = self._preprocess(inputs)\n",
        "    if self.is_regression:\n",
        "      labels = tf.constant([ex[self.config.label_name] for ex in inputs],\n",
        "                           dtype=tf.float32)\n",
        "    else:\n",
        "      labels = tf.constant([\n",
        "          self.config.labels.index(ex[self.config.label_name]) for ex in inputs\n",
        "      ],\n",
        "                           dtype=tf.int64)\n",
        "    # encoded_input is actually a transformers.tokenization_utils.BatchEncoding\n",
        "    # object, which tf.data.Dataset doesn't like. Convert to a regular dict.\n",
        "    return tf.data.Dataset.from_tensor_slices((dict(encoded_input), labels))\n",
        "\n",
        "  def train(self,\n",
        "            train_inputs: List[JsonDict],\n",
        "            validation_inputs: List[JsonDict],\n",
        "            learning_rate=2e-5,\n",
        "            batch_size=32,\n",
        "            num_epochs=3,\n",
        "            keras_callbacks=None):\n",
        "    \"\"\"Run fine-tuning.\"\"\"\n",
        "    train_dataset = self._make_dataset(train_inputs).shuffle(128).batch(\n",
        "        batch_size).repeat(-1)\n",
        "    # Use larger batch for validation since inference is about 1/2 memory usage\n",
        "    # of backprop.\n",
        "    eval_batch_size = 2 * batch_size\n",
        "    validation_dataset = self._make_dataset(validation_inputs).batch(\n",
        "        eval_batch_size)\n",
        "\n",
        "    # Prepare model for training.\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "    if self.is_regression:\n",
        "      loss = tf.keras.losses.MeanSquaredError()\n",
        "      metric = tf.keras.metrics.RootMeanSquaredError(\"rmse\")\n",
        "    else:\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "      metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
        "    self.model.compile(optimizer=opt, loss=loss, metrics=[metric])\n",
        "\n",
        "    steps_per_epoch = len(train_inputs) // batch_size\n",
        "    validation_steps = len(validation_inputs) // eval_batch_size\n",
        "    history = self.model.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=validation_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=keras_callbacks,\n",
        "        verbose=2)\n",
        "    return history\n",
        "\n",
        "  def save(self, path: str):\n",
        "    \"\"\"Save model weights and tokenizer info.\n",
        "    To re-load, pass the path to the constructor instead of the name of a\n",
        "    base model.\n",
        "    Args:\n",
        "      path: directory to save to. Will write several files here.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(path):\n",
        "      os.mkdir(path)\n",
        "    self.tokenizer.save_pretrained(path)\n",
        "    self.model.save_pretrained(path)\n",
        "\n",
        "  def _segment_slicers(self, tokens: List[str]):\n",
        "    \"\"\"Slicers along the tokens dimension for each segment.\n",
        "    For tokens ['[CLS]', a0, a1, ..., '[SEP]', b0, b1, ..., '[SEP]'],\n",
        "    we want to get the slices [a0, a1, ...] and [b0, b1, ...]\n",
        "    Args:\n",
        "      tokens: <string>[num_tokens], including special tokens\n",
        "    Returns:\n",
        "      (slicer_a, slicer_b), slice objects\n",
        "    \"\"\"\n",
        "    try:\n",
        "      split_point = tokens.index(self.tokenizer.sep_token)\n",
        "    except ValueError:\n",
        "      split_point = len(tokens) - 1\n",
        "    slicer_a = slice(1, split_point)  # start after [CLS]\n",
        "    slicer_b = slice(split_point + 1, len(tokens) - 1)  # end before last [SEP]\n",
        "    return slicer_a, slicer_b\n",
        "\n",
        "  def _postprocess(self, output: Dict[str, np.ndarray]):\n",
        "    \"\"\"Per-example postprocessing, on NumPy output.\"\"\"\n",
        "    ntok = output.pop(\"ntok\")\n",
        "    output[\"tokens\"] = self.tokenizer.convert_ids_to_tokens(\n",
        "        output.pop(\"input_ids\")[:ntok])\n",
        "\n",
        "    # Tokens for each segment, individually.\n",
        "    slicer_a, slicer_b = self._segment_slicers(output[\"tokens\"])\n",
        "    output[\"tokens_\" + self.config.text_a_name] = output[\"tokens\"][slicer_a]\n",
        "    if self.config.text_b_name:\n",
        "      output[\"tokens_\" + self.config.text_b_name] = output[\"tokens\"][slicer_b]\n",
        "\n",
        "    # Embeddings for each segment, individually.\n",
        "    output[\"input_embs_\" + self.config.text_a_name] = (\n",
        "        output[\"input_embs\"][slicer_a])\n",
        "    if self.config.text_b_name:\n",
        "      output[\"input_embs_\" + self.config.text_b_name] = (\n",
        "          output[\"input_embs\"][slicer_b])\n",
        "\n",
        "    # Gradients for each segment, individually.\n",
        "    if self.config.compute_grads:\n",
        "      output[\"token_grad_\" +\n",
        "             self.config.text_a_name] = output[\"input_emb_grad\"][slicer_a]\n",
        "      if self.config.text_b_name:\n",
        "        output[\"token_grad_\" +\n",
        "               self.config.text_b_name] = output[\"input_emb_grad\"][slicer_b]\n",
        "      if self.is_regression:\n",
        "        output[\"grad_class\"] = None\n",
        "      else:\n",
        "        # Return the label corresponding to the class index used for gradients.\n",
        "        output[\"grad_class\"] = self.config.labels[output[\"grad_class\"]]\n",
        "\n",
        "    # Process attention.\n",
        "    for key in output:\n",
        "      if not re.match(r\"layer_(\\d+)/attention\", key):\n",
        "        continue\n",
        "      # Select only real tokens, since most of this matrix is padding.\n",
        "      # <float32>[num_heads, max_seq_length, max_seq_length]\n",
        "      # -> <float32>[num_heads, num_tokens, num_tokens]\n",
        "      output[key] = output[key][:, :ntok, :ntok].transpose((0, 2, 1))\n",
        "      # Make a copy of this array to avoid memory leaks, since NumPy otherwise\n",
        "      # keeps a pointer around that prevents the source array from being GCed.\n",
        "      output[key] = output[key].copy()  # pytype: disable=attribute-error\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _scatter_embs(self, passed_input_embs, input_embs, batch_indices,\n",
        "                    offsets):\n",
        "    \"\"\"Scatters custom passed embeddings into the default model embeddings.\n",
        "    Args:\n",
        "      passed_input_embs: <tf.float32>[num_scatter_tokens], the custom passed\n",
        "        embeddings to be scattered into the default model embeddings.\n",
        "      input_embs: the default model embeddings.\n",
        "      batch_indices: the indices of the embeddings to replace in the format\n",
        "        (batch_index, sequence_index).\n",
        "      offsets: the offset from which to scatter the custom embedding (number of\n",
        "        tokens from the start of the sequence).\n",
        "    Returns:\n",
        "      The default model embeddings with scattered custom embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # <float32>[scatter_batch_size, num_tokens, emb_size]\n",
        "    filtered_embs = [emb for emb in passed_input_embs if emb is not None]\n",
        "\n",
        "    # Prepares update values that should be scattered in, i.e. one for each\n",
        "    # of the (scatter_batch_size * num_tokens) word embeddings.\n",
        "    # <np.float32>[scatter_batch_size * num_tokens, emb_size]\n",
        "    updates = np.concatenate(filtered_embs)\n",
        "\n",
        "    # Prepares indices in format (batch_index, sequence_index) for all\n",
        "    # values that should be scattered in, i.e. one for each of the\n",
        "    # (scatter_batch_size * num_tokens) word embeddings.\n",
        "    scatter_indices = []\n",
        "    for (batch_index, sentence_embs, offset) in zip(batch_indices,\n",
        "                                                    filtered_embs, offsets):\n",
        "      for (token_index, emb) in enumerate(sentence_embs):\n",
        "        scatter_indices.append([batch_index, token_index + offset])\n",
        "\n",
        "    # Scatters passed word embeddings into embeddings gathered from tokens.\n",
        "    # <tf.float32>[batch_size, num_tokens + num_special_tokens, emb_size]\n",
        "    return tf.tensor_scatter_nd_update(input_embs, scatter_indices, updates)\n",
        "\n",
        "  def scatter_all_embeddings(self, inputs, input_embs):\n",
        "    \"\"\"Scatters custom passed embeddings for text segment inputs.\n",
        "    Args:\n",
        "      inputs: the model inputs, which contain any custom embeddings to scatter.\n",
        "      input_embs: the default model embeddings.\n",
        "    Returns:\n",
        "      The default model embeddings with scattered custom embeddings.\n",
        "    \"\"\"\n",
        "    # Gets batch indices of any word embeddings that were passed for text_a.\n",
        "    passed_input_embs_a = [ex.get(\"input_embs_\" + self.config.text_a_name)\n",
        "                           for ex in inputs]\n",
        "    batch_indices_a = [index for (index, emb) in enumerate(\n",
        "        passed_input_embs_a) if emb is not None]\n",
        "\n",
        "    # If word embeddings were passed in for text_a, scatter them into the\n",
        "    # embeddings, gathered from the input ids. 1 is passed in as the offset\n",
        "    # for each, since text_a starts at index 1, after the [CLS] token.\n",
        "    if batch_indices_a:\n",
        "      input_embs = self._scatter_embs(\n",
        "          passed_input_embs_a, input_embs, batch_indices_a,\n",
        "          offsets=np.ones(len(batch_indices_a), dtype=np.int64))\n",
        "\n",
        "    if self.config.text_b_name:\n",
        "      # Gets batch indices of any word embeddings that were passed for text_b.\n",
        "      passed_input_embs_b = [ex.get(\"input_embs_\" + self.config.text_b_name)\n",
        "                             for ex in inputs]\n",
        "      batch_indices_b = [\n",
        "          index for (index, emb) in enumerate(passed_input_embs_b)\n",
        "          if emb is not None]\n",
        "\n",
        "      # If word embeddings were also passed in for text_b, scatter them into the\n",
        "      # embeddings gathered from the input ids. The offsets are the [lengths\n",
        "      # of the corresponding text_a embeddings] + 2, since text_b starts after\n",
        "      # [CLS] [text_a tokens] [SEP]. (This assumes that text_b embeddings\n",
        "      # will only be passed together with text_a embeddings.)\n",
        "      if batch_indices_b:\n",
        "        lengths = np.array([len(embed) for embed in passed_input_embs_a\n",
        "                            if embed is not None])\n",
        "        input_embs = self._scatter_embs(\n",
        "            passed_input_embs_b, input_embs, batch_indices_b,\n",
        "            offsets=(lengths + 2))\n",
        "    return input_embs\n",
        "\n",
        "  ##\n",
        "  # LIT API implementation\n",
        "  def max_minibatch_size(self):\n",
        "    return self.config.inference_batch_size\n",
        "\n",
        "  def predict_minibatch(self, inputs: Iterable[JsonDict]):\n",
        "    # Use watch_accessed_variables to save memory by having the tape do nothing\n",
        "    # if we don't need gradients.\n",
        "    with tf.GradientTape(\n",
        "        watch_accessed_variables=self.config.compute_grads) as tape:\n",
        "      encoded_input = self._preprocess(inputs)\n",
        "\n",
        "      # Gathers word embeddings from BERT model embedding layer using input ids\n",
        "      # of the tokens.\n",
        "      input_ids = encoded_input[\"input_ids\"]\n",
        "\n",
        "      word_embeddings = None\n",
        "      \n",
        "      if hasattr(self.model, 'albert'):\n",
        "        word_embeddings = self.model.albert.embeddings.word_embeddings\n",
        "      else:\n",
        "        word_embeddings = self.model.bert.embeddings.word_embeddings\n",
        "      \n",
        "      # <tf.float32>[batch_size, num_tokens, emb_size]\n",
        "      input_embs = tf.gather(word_embeddings, input_ids)\n",
        "\n",
        "      # Scatter in any passed in embeddings.\n",
        "      # <tf.float32>[batch_size, num_tokens, emb_size]\n",
        "      input_embs = self.scatter_all_embeddings(inputs, input_embs)\n",
        "\n",
        "      tape.watch(input_embs)  # Watch input_embs for gradient calculation.\n",
        "\n",
        "      model_inputs = encoded_input.copy()\n",
        "      model_inputs[\"input_ids\"] = None\n",
        "      logits, embs, attentions = self.model(model_inputs,\n",
        "                                            inputs_embeds=input_embs,\n",
        "                                            training=False)\n",
        "\n",
        "      batched_outputs = {\n",
        "          \"input_ids\": encoded_input[\"input_ids\"],\n",
        "          \"ntok\": tf.reduce_sum(encoded_input[\"attention_mask\"], axis=1),\n",
        "          \"cls_emb\": embs[-1][:, 0],  # last layer, first token\n",
        "          \"input_embs\": input_embs\n",
        "      }\n",
        "      assert len(attentions) == self.model.config.num_hidden_layers\n",
        "      for i, layer_attention in enumerate(attentions):\n",
        "        batched_outputs[f\"layer_{i}/attention\"] = layer_attention\n",
        "\n",
        "      if self.is_regression:\n",
        "        # <tf.float32>[batch_size]\n",
        "        batched_outputs[\"score\"] = tf.squeeze(logits, axis=-1)\n",
        "        scalar_pred_for_gradients = batched_outputs[\"score\"]\n",
        "      else:\n",
        "        # <tf.float32>[batch_size, num_labels]\n",
        "        batched_outputs[\"probas\"] = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "        # If a class for the gradients has been specified in the input,\n",
        "        # calculate gradients for that class. Otherwise, calculate gradients for\n",
        "        # the arg_max class.\n",
        "        arg_max = tf.math.argmax(batched_outputs[\"probas\"], axis=-1).numpy()\n",
        "        grad_classes = [ex.get(\"grad_class\", arg_max[i]) for (i, ex) in\n",
        "                        enumerate(inputs)]\n",
        "        # Convert the class names to indices if needed.\n",
        "        grad_classes = [self.config.labels.index(label)\n",
        "                        if isinstance(label, str) else label\n",
        "                        for label in grad_classes]\n",
        "\n",
        "        gather_indices = list(enumerate(grad_classes))\n",
        "        # <tf.float32>[batch_size]\n",
        "        scalar_pred_for_gradients = tf.gather_nd(batched_outputs[\"probas\"],\n",
        "                                                 gather_indices)\n",
        "        if self.config.compute_grads:\n",
        "          batched_outputs[\"grad_class\"] = tf.convert_to_tensor(grad_classes)\n",
        "\n",
        "    # Request gradients after the tape is run.\n",
        "    # Note: embs[0] includes position and segment encodings, as well as subword\n",
        "    # embeddings.\n",
        "    if self.config.compute_grads:\n",
        "      # <tf.float32>[batch_size, num_tokens, emb_dim]\n",
        "      batched_outputs[\"input_emb_grad\"] = tape.gradient(\n",
        "          scalar_pred_for_gradients, input_embs)\n",
        "\n",
        "    detached_outputs = {k: v.numpy() for k, v in batched_outputs.items()}\n",
        "    # Sequence of dicts, one per example.\n",
        "    unbatched_outputs = utils.unbatch_preds(detached_outputs)\n",
        "    return map(self._postprocess, unbatched_outputs)\n",
        "\n",
        "  def input_spec(self) -> Spec:\n",
        "    ret = {}\n",
        "    ret[self.config.text_a_name] = lit_types.TextSegment()\n",
        "    if self.config.text_b_name:\n",
        "      ret[self.config.text_b_name] = lit_types.TextSegment()\n",
        "    if self.is_regression:\n",
        "      ret[self.config.label_name] = lit_types.RegressionScore(required=False)\n",
        "    else:\n",
        "      ret[self.config.label_name] = lit_types.CategoryLabel(\n",
        "          required=False, vocab=self.config.labels)\n",
        "    # The input_embs_ and grad_class fields are used for Integrated Gradients.\n",
        "    ret[\"input_embs_\" + self.config.text_a_name] = lit_types.TokenEmbeddings(\n",
        "        align=\"tokens\", required=False)\n",
        "    if self.config.text_b_name:\n",
        "      ret[\"input_embs_\" + self.config.text_b_name] = lit_types.TokenEmbeddings(\n",
        "          align=\"tokens\", required=False)\n",
        "    ret[\"grad_class\"] = lit_types.CategoryLabel(required=False,\n",
        "                                                vocab=self.config.labels)\n",
        "    return ret\n",
        "\n",
        "  def output_spec(self) -> Spec:\n",
        "    ret = {\"tokens\": lit_types.Tokens()}\n",
        "    ret[\"tokens_\" + self.config.text_a_name] = lit_types.Tokens()\n",
        "    if self.config.text_b_name:\n",
        "      ret[\"tokens_\" + self.config.text_b_name] = lit_types.Tokens()\n",
        "    if self.is_regression:\n",
        "      ret[\"score\"] = lit_types.RegressionScore(parent=self.config.label_name)\n",
        "    else:\n",
        "      ret[\"probas\"] = lit_types.MulticlassPreds(\n",
        "          parent=self.config.label_name,\n",
        "          vocab=self.config.labels,\n",
        "          null_idx=self.config.null_label_idx)\n",
        "    ret[\"cls_emb\"] = lit_types.Embeddings()\n",
        "\n",
        "    # The input_embs_ and grad_class fields are used for Integrated Gradients.\n",
        "    ret[\"input_embs_\" + self.config.text_a_name] = lit_types.TokenEmbeddings(\n",
        "        align=\"tokens_\" + self.config.text_a_name)\n",
        "    if self.config.text_b_name:\n",
        "      ret[\"input_embs_\" + self.config.text_b_name] = lit_types.TokenEmbeddings(\n",
        "          align=\"tokens_\" + self.config.text_b_name)\n",
        "\n",
        "    # Gradients, if requested.\n",
        "    if self.config.compute_grads:\n",
        "      ret[\"grad_class\"] = lit_types.CategoryLabel(required=False,\n",
        "                                                  vocab=self.config.labels)\n",
        "      ret[\"token_grad_\" + self.config.text_a_name] = lit_types.TokenGradients(\n",
        "          align=\"tokens_\" + self.config.text_a_name,\n",
        "          grad_for=\"input_embs_\" + self.config.text_a_name,\n",
        "          grad_target=\"grad_class\")\n",
        "      if self.config.text_b_name:\n",
        "        ret[\"token_grad_\" + self.config.text_b_name] = lit_types.TokenGradients(\n",
        "            align=\"tokens_\" + self.config.text_b_name,\n",
        "            grad_for=\"input_embs_\" + self.config.text_b_name,\n",
        "            grad_target=\"grad_class\")\n",
        "\n",
        "    # Attention heads, one field for each layer.\n",
        "    for i in range(self.model.config.num_hidden_layers):\n",
        "      ret[f\"layer_{i}/attention\"] = lit_types.AttentionHeads(\n",
        "          align=(\"tokens\", \"tokens\"))\n",
        "    return ret\n",
        "\n",
        "\n",
        "class SST2Model(GlueModel):\n",
        "  \"\"\"Classification model on SST-2.\"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kw):\n",
        "    super().__init__(\n",
        "        *args,\n",
        "        text_a_name=\"sentence\",\n",
        "        text_b_name=None,\n",
        "        labels=[\"0\", \"1\"],\n",
        "        null_label_idx=0,\n",
        "        **kw)\n",
        "\n",
        "\n",
        "class MNLIModel(GlueModel):\n",
        "  \"\"\"Classification model on MultiNLI.\"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kw):\n",
        "    super().__init__(\n",
        "        *args,\n",
        "        text_a_name=\"premise\",\n",
        "        text_b_name=\"hypothesis\",\n",
        "        labels=[\"entailment\", \"neutral\", \"contradiction\"],\n",
        "        **kw)\n",
        "    \n",
        "\n",
        "class RTEModel(GlueModel):\n",
        "  \"\"\"Classification model on RTE.\"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kw):\n",
        "    super().__init__(\n",
        "        *args,\n",
        "        text_a_name=\"sentence1\",\n",
        "        text_b_name=\"sentence2\",\n",
        "        labels=[\"entailment\", \"not_entailment\"],\n",
        "        **kw)\n",
        "\n",
        "\n",
        "class STSBModel(GlueModel):\n",
        "  \"\"\"Regression model on STS-B.\"\"\"\n",
        "\n",
        "  def __init__(self, *args, **kw):\n",
        "    super().__init__(\n",
        "        *args,\n",
        "        text_a_name=\"sentence1\",\n",
        "        text_b_name=\"sentence2\",\n",
        "        labels=None,\n",
        "        **kw)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting glue_models/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRssGk0PKjR1",
        "outputId": "c251a9c0-3f33-4f01-b43d-ac349909cd31"
      },
      "source": [
        "%%writefile lit.py\n",
        "\n",
        "# Lint as: python3\n",
        "r\"\"\"Quick-start demo for a sentiment analysis model.\n",
        "This demo fine-tunes a small Transformer (BERT-tiny) on the Stanford Sentiment\n",
        "Treebank (SST-2), and starts a LIT server.\n",
        "To run locally:\n",
        "  python -m lit_nlp.examples.quickstart_sst_demo --port=5432\n",
        "Training should take less than 5 minutes on a single GPU. Once you see the\n",
        "ASCII-art LIT logo, navigate to localhost:5432 to access the demo UI.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "\n",
        "from lit_nlp import dev_server\n",
        "from lit_nlp import server_flags\n",
        "from lit_nlp.components import word_replacer\n",
        "from lit_nlp.examples.datasets import glue\n",
        "\n",
        "from glue_models import *\n",
        "\n",
        "# NOTE: additional flags defined in server_flags.py\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_list(\n",
        "    \"models\", [\n",
        "               \"textattack/bert-base-uncased-SST-2\",\n",
        "               \"textattack/albert-base-v2-SST-2\",\n",
        "               \"textattack/bert-base-uncased-RTE\",\n",
        "               \"textattack/albert-base-v2-RTE\",\n",
        "    ],\n",
        "    \"Models to load.\"\n",
        ")\n",
        "\n",
        "def main(_):\n",
        "  # Load model\n",
        "\n",
        "  models = {}\n",
        "  for model_name_or_path in FLAGS.models:\n",
        "    # Ignore path prefix, if using /path/to/<model_name> to load from a\n",
        "    # specific directory rather than the default shortcut.\n",
        "    model_name = os.path.basename(model_name_or_path)\n",
        "\n",
        "    if 'SST-2' in model_name:\n",
        "      models[model_name] = SST2Model(model_name_or_path)\n",
        "    elif 'RTE' in model_name:\n",
        "      models[model_name] = RTEModel(model_name_or_path)\n",
        "\n",
        "  datasets = {}\n",
        "\n",
        "  datasets[\"sst_dev\"] = glue.SST2Data(\"validation\")\n",
        "  datasets[\"rte_dev\"] = glue.RTEData(\"validation\")\n",
        "\n",
        "  # Start the LIT server. See server_flags.py for server options.\n",
        "  generators = {\"word_replacer\": word_replacer.WordReplacer()}\n",
        "\n",
        "  lit_server = dev_server.Server(models, datasets, generators=generators, **server_flags.get_flags())\n",
        "  lit_server.serve()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run(main)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting lit.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aLffr3swj7d"
      },
      "source": [
        "# Start ngrok\n",
        "get_ipython().system_raw('./ngrok http 5432 &')"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlDGCXa8hixa",
        "outputId": "cf4a9649-91ab-4dbc-c5f8-bac451711ce5"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://bf32978cfc03.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXFyaco3ujCI",
        "outputId": "606bb3dd-c465-4304-e240-f52ab333898c"
      },
      "source": [
        "!python lit.py --port=5432"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "I0128 22:51:11.266665 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.266738 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.266814 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.266885 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:11.267000 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:11.267223 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.267858 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:11.268100 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.268183 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.268249 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.268324 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.268410 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:11.268542 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:11.268765 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.269408 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:11.269703 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.269792 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.269860 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.269934 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.270047 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:11.270132 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:11.271694 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:11.273217 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.273880 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:11.274124 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.274206 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.274273 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.274347 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.274445 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:11.274588 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:11.274861 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.275517 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:11.275812 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.275902 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.275970 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.276049 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.276131 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:11.276243 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:11.276515 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.277144 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:11.277459 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.277564 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.277634 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.277711 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.277863 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:11.278333 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.279004 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:11.279308 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.279442 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.279535 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.279618 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.279791 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:11.280321 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.280991 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:11.281302 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.281418 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.281502 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.281586 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.281767 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:11.282217 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.282873 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:11.283172 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.283281 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.283360 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.283486 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.283576 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:11.283738 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:11.536596 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.586931 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:11.587534 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.587650 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.587722 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.587815 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.587954 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:11.588057 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:11.589754 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:11.591562 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.592303 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=umap\n",
            "I0128 22:51:11.594055 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.594154 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 280 inputs\n",
            "I0128 22:51:11.594226 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.594307 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.594410 140717416466304 projection.py:207] Projection request: instance key: frozenset({('dataset_name', 'rte_dev'), ('proj_kw', frozenset({('n_components', 3)})), ('field_name', 'cls_emb'), ('model_name', 'bert-base-uncased-RTE')})\n",
            "I0128 22:51:11.595148 140717416466304 caching.py:224] CachingModelWrapper 'frozenset({('dataset_name', 'rte_dev'), ('proj_kw', frozenset({('n_components', 3)})), ('field_name', 'cls_emb'), ('model_name', 'bert-base-uncased-RTE')})': misses (dataset=): []\n",
            "I0128 22:51:11.595278 140717416466304 caching.py:226] CachingModelWrapper 'frozenset({('dataset_name', 'rte_dev'), ('proj_kw', frozenset({('n_components', 3)})), ('field_name', 'cls_emb'), ('model_name', 'bert-base-uncased-RTE')})': 0 misses out of 280 inputs\n",
            "I0128 22:51:11.595350 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.595448 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.596831 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=umap HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.597497 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:11.597806 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.597902 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.597968 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.598044 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.598156 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:11.598249 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:11.599738 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:11.601244 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.601928 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:11.602228 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:11.602320 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:11.602406 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:11.602501 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:11.602579 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:11.602728 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:11.925682 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:11] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:11.999508 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:11.999893 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:12.000007 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:12.000080 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:12.000169 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:12.001522 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:12] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:12.002498 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:12.002894 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:12.003044 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:12.003146 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:12.003250 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:12.004353 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:12] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.494038 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:13.494482 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.494593 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.494666 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.494757 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.495733 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.499612 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:13.499858 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.499940 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.500008 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.500087 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.500930 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.501623 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:13.501874 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.501956 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.502023 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.502098 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.502170 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:13.502299 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:13.502579 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.562580 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:13.562901 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.563004 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.563080 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.563160 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.563278 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:13.563381 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.565937 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.567999 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.570598 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:13.570845 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.570933 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.571006 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.571085 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.571162 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:13.571286 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:13.571561 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.572257 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:13.572563 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.572658 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.572730 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.572817 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.572894 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:13.573017 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:13.573248 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.573939 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:13.574189 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.574284 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.574356 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.574459 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.574534 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:13.574658 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:13.575021 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.575743 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:13.575994 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.576075 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.576141 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.576214 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.576288 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:13.576420 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:13.576652 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.577284 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:13.577561 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.577644 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.577716 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.577795 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.577903 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:13.577986 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.579341 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.580905 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.581562 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:13.581801 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.581881 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.581948 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.582023 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.582175 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:13.582731 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.585949 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:13.586244 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.586329 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.586423 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.586501 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.586663 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:13.587156 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.587849 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:13.588097 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.588176 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.588243 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.588322 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.588444 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:13.588541 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.589854 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.594133 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.594791 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:13.595073 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.595168 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.595264 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.595343 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.595577 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:13.596050 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.596730 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:13.597036 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.597134 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.597201 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.597284 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.597410 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:13.597512 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.599040 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:13.600759 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.601417 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:13.601690 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.601774 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.601993 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.602106 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.602286 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:13.602808 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:13] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:13.603534 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:13.603822 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:13.603914 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:13.603998 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:13.604094 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:13.604175 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:13.604326 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:14.009006 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:14.347347 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:14.347781 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:14.347890 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:14.347986 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:14.348084 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:14.348162 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:14.348295 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:14.348658 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:14.349426 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:14.349738 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:14.349841 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:14.349917 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:14.350003 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:14.350081 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:14.350256 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:14.731753 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:14.881258 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:14.881730 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:14.881849 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:14.881959 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:14.882099 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:14.883543 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:14.884436 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:14.884757 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:14.884885 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:14.884972 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:14.885169 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:14.885320 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:14.885467 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:14.887878 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:14.889523 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:14.890179 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:14.890498 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:14.890589 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:14.890668 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:14.890756 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:14.891828 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:14.892472 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:14.892754 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:14.892861 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:14.892943 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:14.893048 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:14.893178 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:14.893295 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:14.894864 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:14.896920 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:14] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.562743 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:17.563151 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.563251 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.563323 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.563432 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.564400 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.567180 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:17.567463 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.567550 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.567619 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.567702 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.568760 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.569287 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:17.569548 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.569639 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.569717 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.569801 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.569883 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:17.570022 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:17.570289 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.619203 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:17.624894 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.625020 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.625144 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.625250 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.625416 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:17.625560 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:17.628018 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:17.629776 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.630497 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:17.630779 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.630866 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.630938 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.631018 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.631133 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:17.631232 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:17.633201 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:17.634722 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.635406 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:17.635652 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.635735 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.635801 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.635873 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.635943 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:17.636060 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:17.636294 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.637041 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:17.637306 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.637410 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.637485 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.637564 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.637640 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:17.637777 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:17.638047 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.638722 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:17.639029 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.639145 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.639249 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.639347 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.639444 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:17.639569 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:17.639808 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.644350 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:17.644615 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.644704 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.644776 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.644855 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.645067 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:17.645687 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.646486 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:17.646737 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.646821 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.646891 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.646979 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.647129 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:17.647641 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.648347 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:17.648684 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.648802 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.648891 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.648993 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.649092 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:17.649233 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:17.649725 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.650446 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:17.650712 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.650797 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.650869 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.650948 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.651117 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:17.651729 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.652431 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:17.652737 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.652849 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.652930 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.653028 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.653125 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:17.653305 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:17.888463 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.995399 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:17.995798 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.995893 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.995964 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.996073 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.996302 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:17.996942 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:17] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:17.997706 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:17.998010 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:17.998103 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:17.998176 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:17.998259 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:17.998338 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:17.998505 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:18.327519 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.402924 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:18.403366 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.403515 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.403603 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.403697 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.405057 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.405964 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:18.406349 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.406473 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.406563 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.406656 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.406785 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:18.406898 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.408883 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.410998 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.411707 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:18.412019 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.412130 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.412213 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.412313 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.412460 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:18.412586 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.413937 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.415471 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.416116 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:18.416448 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.416551 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.416626 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.416718 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.416807 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:18.416934 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:18.417258 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.417905 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:18.418165 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.418250 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.418317 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.418420 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.419158 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.419819 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:18.420103 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.420192 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.420273 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.420350 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.420484 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:18.420573 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.421921 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.423749 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.424432 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:18.424752 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.424861 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.424944 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.425101 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.425253 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:18.425410 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.427138 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.429110 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.917015 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:18.917579 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.917713 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.917802 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.917911 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.919461 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.922045 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:18.922412 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.922537 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.922622 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.922740 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.923971 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.927647 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:18.927921 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.928029 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.928111 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.928207 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.928298 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:18.928463 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:18.928745 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.972254 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:18.972616 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.972739 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.972825 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.972921 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.973065 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:18.973176 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.975658 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.981050 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.981968 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:18.982269 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.982355 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.982471 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.982558 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.982641 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:18.982775 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:18.983051 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.983788 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:18.984141 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.984262 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.984343 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.984485 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.984575 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:18.984713 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:18.984963 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.985776 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:18.986073 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.986193 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.986262 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.986339 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.986467 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:18.986557 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.988156 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:18.990059 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:18.990719 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:18.991066 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:18.991166 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:18.991237 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:18.991318 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:18.991407 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:18.991564 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:19.356208 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.476440 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:19.476888 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.477019 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.477100 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.477205 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.477444 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:19.478108 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.479770 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:19.480032 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.480117 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.480188 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.480270 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.480344 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:19.480485 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:19.480796 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.481490 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:19.481801 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.481910 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.481997 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.482100 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.482332 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:19.483067 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.483759 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:19.484076 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.484184 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.484258 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.484342 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.484451 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:19.484587 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:19.484837 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.485623 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:19.485920 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.486021 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.486110 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.486209 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.486364 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:19.486896 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.487566 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:19.487896 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.487997 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.488070 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.488147 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.488308 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:19.488827 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.489508 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:19.489817 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.489914 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.489991 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.490068 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.490179 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:19.490268 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:19.492285 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:19.494004 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:19.494664 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:19.494958 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:19.495056 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:19.495130 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:19.495214 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:19.495298 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:19.495466 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:19.960363 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:19] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:20.292603 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:20.293051 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:20.293193 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:20.293282 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:20.293396 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:20.293558 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:20.293701 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:20.296241 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:20.298399 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:20] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:20.299144 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:20.299445 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:20.299542 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:20.299610 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:20.299695 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:20.300571 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:20] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:20.301263 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:20.301596 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:20.301684 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:20.301753 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:20.301834 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:20.302619 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:20] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:20.303445 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:20.303751 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:20.303850 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:20.303921 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:20.304011 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:20.304095 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:20.304228 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:20.304629 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:20] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:20.305400 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:20.305739 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:20.305839 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:20.305919 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:20.306016 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:20.306153 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:20.306266 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:20.307682 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:20.309441 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:20] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:20.310664 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:20.310936 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:20.311038 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:20.311114 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:20.311191 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:20.311303 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:20.311411 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:20.313325 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:20.315857 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:20] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.847013 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:21.847489 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.847593 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.847664 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.847754 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.849014 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.869047 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:21.869322 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.869441 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.869516 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.869606 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.869686 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:21.869811 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:21.870070 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.870754 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:21.871020 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.871104 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.871167 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.871241 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.872065 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.897626 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:21.897863 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.897944 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.898021 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.898096 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.898203 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:21.898286 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:21.900666 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:21.902299 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.902959 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:21.903207 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.903288 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.903352 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.903444 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.903512 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:21.903626 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:21.903842 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.904493 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:21.904767 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.904848 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.904910 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.905004 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.905114 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:21.905249 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:21.905546 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.906280 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:21.906558 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.906641 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.906704 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.906775 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.906878 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:21.906957 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:21.908315 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:21.909865 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.917524 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:21.917754 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.917834 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.917900 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.917973 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.918131 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:21.918617 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.919267 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:21.919608 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.919725 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.919815 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.919918 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.920019 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:21.920164 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:21.920466 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.921101 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:21.921356 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.921459 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.921530 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.921603 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.921691 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:21.921818 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:21.922111 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.922781 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:21.923108 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.923222 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.923433 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.923566 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.924622 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:21.925492 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.926283 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:21.926569 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.926654 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.926720 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.926791 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.926937 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:21.927518 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.928170 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:21.928473 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.928564 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.928635 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.928714 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.928878 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:21.929437 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:21] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:21.930093 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:21.930355 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:21.930460 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:21.930536 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:21.930611 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:21.930680 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:21.930829 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:22.224768 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.297754 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:22.298224 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.298367 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.298460 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.298561 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.298645 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:22.298811 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:22.569325 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.675379 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:22.675915 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.676034 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.676103 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.676188 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.676316 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:22.676435 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.678702 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.680757 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.681569 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:22.681905 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.682021 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.682103 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.682205 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.682333 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:22.682434 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.684187 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.685882 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.686632 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:22.686930 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.687030 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.687102 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.687177 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.687249 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:22.687371 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:22.687613 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.688249 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:22.688551 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.688640 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.688710 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.688782 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.689588 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.690241 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:22.690577 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.690684 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.690755 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.690838 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.690953 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:22.691054 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.692794 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.694643 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.695315 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:22.695646 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.695756 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.695841 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.695936 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.696069 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:22.696174 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.698184 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.699910 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.700595 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:22.700882 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.700973 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.701051 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.701137 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.701911 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.702590 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:22.702908 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.703012 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.703083 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.703167 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.703953 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.704632 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:22.704934 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.705099 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.705204 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.705320 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.705427 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:22.705568 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:22.705922 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.706652 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:22.706957 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.707082 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.707166 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.707255 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.708328 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.709011 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:22.709319 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.709427 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.709514 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.709608 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.709733 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:22.709820 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.711310 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.712904 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.713595 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:22.713895 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.714005 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.714085 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.714186 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.714274 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:22.714404 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:22.714634 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.715272 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:22.715556 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.715641 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.715706 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.715778 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.715927 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:22.716453 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.717111 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:22.717410 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.717513 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.717585 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.717663 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.717733 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:22.717845 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:22.718104 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.718764 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:22.719080 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.719191 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.719270 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.719355 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.719469 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:22.719609 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:22.719899 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.720568 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:22.720873 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.720986 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.721067 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.721151 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.721240 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:22.721397 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:22.721702 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.722354 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:22.722681 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.722788 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.722865 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.722953 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.723092 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:22.723195 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.724758 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:22.726710 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:22] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:22.727360 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:22.727686 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:22.762121 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:22.762380 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:22.762646 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:22.762876 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:22.763290 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:23.194301 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:23] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:23.344806 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:23.345492 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:23.345654 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:23.345746 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:23.345872 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:23.346049 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:23.346239 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:23.348349 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:23.350262 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:23] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:23.351034 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:23.351329 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:23.351432 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:23.351505 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:23.351580 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:23.351687 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:23.351768 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:23.353112 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:23.354666 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:23] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:23.355316 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:23.355643 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:23.355734 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:23.355801 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:23.355880 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:23.356051 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:23.356549 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:23] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:23.357197 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:23.357507 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:23.357596 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:23.357664 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:23.357738 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:23.357912 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:23.358365 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:23] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:23.359093 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:23.359426 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:23.359539 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:23.359606 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:23.359683 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:23.359757 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:23.359900 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:23.838619 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:23] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.110471 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:24.111038 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.111178 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.111256 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.111365 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.112687 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.113493 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:24.113772 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.113861 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.113927 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.114005 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.114108 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:24.114189 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.115813 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.117438 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.118115 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:24.118367 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.118475 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.118543 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.118620 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.118781 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:24.119305 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.119957 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:24.120232 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.120313 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.120373 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.120496 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.120626 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:24.120733 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.122225 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.123797 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.124476 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:24.124722 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.124801 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.124861 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.124934 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.125016 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:24.125135 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:24.125349 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.125994 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:24.126240 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.126318 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.126376 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.126470 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.127196 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.737641 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:24.742527 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.742738 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.742903 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.743062 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.744638 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.745492 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:24.745841 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.745949 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.746017 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.746094 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.746920 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.747729 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:24.748044 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.748167 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.748233 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.748308 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.748380 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:24.748532 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:24.748766 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.749428 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:24.749678 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.749760 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.749820 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.749887 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.749988 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:24.750067 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.751640 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.753291 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.754070 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:24.754366 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.754501 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.754911 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.755052 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.755161 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:24.755299 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:24.755562 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.756249 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:24.756608 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.756726 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.756827 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.756900 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.757008 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:24.757086 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.758486 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.760499 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.761179 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:24.761533 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.761646 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.761734 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.761822 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.761898 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:24.762029 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:24.762274 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.763204 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:24.764276 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.764368 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.764455 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.764536 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.764689 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:24.765507 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.766220 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:24.766628 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.766766 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.766899 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.767050 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.767204 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:24.767400 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:24.767762 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.768537 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:24.768911 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.769025 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.769128 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.769226 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.769321 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:24.769535 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:24.769855 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.770628 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:24.770950 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.771069 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.771156 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.771250 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.771465 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:24.772178 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.772881 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:24.773194 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.773310 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.773412 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.773527 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.773715 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:24.774194 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.775635 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:24.775864 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.775944 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.776005 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.776075 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.776177 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:24.776266 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.778143 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:24.780083 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:24] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:24.780782 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:24.781108 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:24.781220 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:24.781308 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:24.781420 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:24.781517 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:24.781648 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:25.099053 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.152899 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:25.153259 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.153354 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.153439 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.153529 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.154464 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.155242 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:25.155554 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.155640 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.155699 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.155769 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.155875 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:25.155957 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:25.157553 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:25.159192 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.159879 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:25.160135 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.160218 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.160278 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.160346 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.160461 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:25.160551 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:25.161839 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:25.163348 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.164036 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:25.164297 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.164379 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.164464 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.164550 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.165286 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.165945 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:25.166181 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.166260 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.166319 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.166402 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.166486 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:25.166621 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:25.421492 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.495323 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:25.495728 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.495833 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.495934 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.496082 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.496208 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:25.496409 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:25.496758 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.497568 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:25.497869 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.497968 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.498045 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.498135 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.498331 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:25.498943 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:25.499640 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:25.499970 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:25.500078 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:25.500168 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:25.500262 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:25.500406 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:25.500534 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:25.502689 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:25.504235 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.705977 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:30.708957 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.709095 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.709167 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.709276 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.710279 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.714183 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:30.714560 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.714679 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.714762 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.714853 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.714938 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:30.715077 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:30.715446 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.725732 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:30.725996 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.726082 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.726147 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.726230 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.727105 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.752966 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:30.753224 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.753307 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.753376 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.753481 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.753603 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:30.753687 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:30.755383 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:30.757004 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.765565 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:30.765799 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.765879 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.765938 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.766006 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.766071 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:30.766183 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:30.766425 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.767048 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:30.767279 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.767358 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.767437 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.767512 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.767582 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:30.767694 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:30.767910 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.768533 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:30.768768 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.768846 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.768904 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.768970 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.769072 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:30.769148 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:30.770487 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:30.772010 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.772681 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:30.772926 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.773005 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.773064 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.773132 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.773278 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:30.773800 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.774452 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:30.774695 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.774773 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.774830 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.774896 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.774965 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:30.775077 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:30.775366 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.776041 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:30.777801 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.777917 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.777999 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.778080 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.778151 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:30.778263 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:30.778535 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.779155 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:30.779413 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.779512 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.779582 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.779656 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.779816 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:30.780346 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.781005 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:30.781248 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.781326 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.781399 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.781482 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.781620 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:30.782075 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:30] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:30.782730 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:30.782978 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:30.783055 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:30.783112 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:30.783177 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:30.783240 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:30.783370 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:31.127346 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:31] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:31.376167 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:31.376577 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:31.376683 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:31.376747 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:31.376828 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:31.377016 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:31.377655 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:31] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:31.378404 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:31.378673 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:31.378754 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:31.378813 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:31.378883 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:31.378989 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:31.379076 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:31.381115 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:31.383229 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:31] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:31.383919 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:31.384189 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:31.384294 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:31.384355 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:31.384480 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:31.385353 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:31] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:31.386041 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:31.386307 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:31.386403 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:31.386473 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:31.386549 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:31.386650 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:31.386728 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:31.388071 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:31.389589 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:31] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:31.390224 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:31.390511 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:31.390595 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:31.390657 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:31.390727 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:31.390796 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:31.390964 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:31.863708 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:31] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.332508 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:32.333032 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.333184 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.333303 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.333436 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.334669 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.335444 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:32.335736 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.335816 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.335875 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.335946 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.336051 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:32.336131 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.337774 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.339360 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.340060 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:32.340349 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.340462 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.340533 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.340610 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.340681 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:32.340807 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:32.341095 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.341743 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:32.342044 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.342152 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.342227 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.342310 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.342435 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:32.342543 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.344109 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.345652 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.346293 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:32.346597 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.346687 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.346753 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.346829 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.347574 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.348210 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:32.348517 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.348604 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.348669 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.348743 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.349466 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.350105 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:32.350378 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.350503 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.350571 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.350646 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.350717 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:32.350834 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:32.351045 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.351689 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:32.351955 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.352040 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.352107 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.352184 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.352253 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:32.352364 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:32.352603 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.353224 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:32.353580 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.353683 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.353767 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.353853 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.353925 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:32.354067 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:32.354315 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.355006 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:32.355277 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.355366 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.355446 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.355525 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.355679 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:32.356165 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.356960 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:32.357207 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.357286 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.357345 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.357425 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.357585 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:32.358086 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.358766 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:32.359074 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.359176 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.359244 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.359320 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.359497 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:32.359994 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.360669 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:32.360960 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.361063 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.361127 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.361201 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.361300 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:32.361380 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.362770 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.364309 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.365040 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:32.365287 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.365366 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.365447 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.365536 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.365602 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:32.365711 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:32.365912 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.366545 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:32.366773 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.366850 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.366907 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.366973 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.367037 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:32.367146 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:32.367350 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.368000 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:32.368247 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.368325 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.368382 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.368472 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.368621 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:32.369157 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.369833 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:32.370080 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.370159 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.370218 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.370284 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.370383 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:32.370496 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.371874 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:32.373457 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:32.374144 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:32.374427 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:32.374547 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:32.374618 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:32.374695 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:32.374765 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:32.404698 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:32.838194 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:32] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.142837 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:33.143219 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.143319 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.143397 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.143482 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.143563 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:33.143712 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:33.507144 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.665996 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:33.666415 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.666504 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.666569 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.666648 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.666791 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:33.666907 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.668943 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.670580 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.671314 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:33.671636 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.671725 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.671800 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.671878 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.671984 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:33.672064 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.673363 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.675368 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.678802 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:33.679125 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.679244 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.679318 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.679416 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.679515 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:33.679658 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:33.679980 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.680696 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:33.680990 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.681085 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.681154 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.681231 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.682428 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.683099 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:33.683427 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.683531 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.683601 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.683681 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.685355 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.686214 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:33.686763 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.686883 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.686977 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.687079 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.687216 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:33.687361 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.689522 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.691973 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:33.692694 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:33.693015 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:33.693123 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:33.693205 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:33.693319 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:33.693470 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:33.693577 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.695698 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:33.698420 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:33] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.314702 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:34.315104 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.315202 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.315264 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.315368 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.316485 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.317240 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:34.317559 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.317675 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.317762 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.317863 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.318767 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.319450 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:34.320038 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.320152 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.320232 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.320311 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.320383 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:34.320515 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:34.320742 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.374364 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:34.374686 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.374779 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.374846 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.374922 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.375046 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:34.375137 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:34.377184 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:34.379296 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.380608 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:34.380905 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.380996 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.381060 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.381131 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.381197 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:34.381327 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:34.381603 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.382707 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:34.382991 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.383077 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.383139 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.383209 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.383326 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:34.383435 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:34.385203 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:34.387351 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.388495 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:34.388767 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.388850 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.388911 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.388988 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.389055 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:34.389172 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:34.389436 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.390536 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:34.390833 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.390925 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.391000 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.391076 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.391292 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:34.391989 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.393110 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:34.393380 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.393479 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.393544 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.393615 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.393680 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:34.393793 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:34.394049 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.395135 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:34.395423 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.395515 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.395579 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.395650 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.395716 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:34.395831 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:34.396084 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.398339 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:34.403567 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.403672 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.403738 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.403815 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.403976 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:34.404542 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.405189 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:34.405461 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.405553 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.405620 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.405716 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.405870 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:34.406379 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.407006 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:34.407270 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.407350 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.407434 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.407533 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.407618 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:34.407776 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:34.766140 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.938992 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:34.939380 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.939517 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.939610 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.939697 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.939897 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:34.940518 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.941307 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:34.941607 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.941693 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.941760 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.941840 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.941951 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:34.942035 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:34.943854 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:34.945493 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:34.946128 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:34.946442 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:34.946544 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:34.946614 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:34.946692 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:34.946763 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:34.946898 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:35.396265 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.740601 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:35.741032 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.741146 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.741216 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.741307 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.742333 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.743119 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:35.743456 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.743583 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.743661 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.743748 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.743875 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:35.743983 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.745834 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.747448 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.748108 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:35.748362 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.748468 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.748540 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.748619 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.749325 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.750002 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:35.750281 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.750404 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.750482 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.750566 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.750639 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:35.750755 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:35.750981 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.751627 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:35.751940 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.752047 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.752126 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.752223 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.752327 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:35.752440 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.754002 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.755650 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.756401 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:35.756711 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.756801 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.756869 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.756945 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.757048 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:35.757127 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.758521 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.760316 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.760989 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:35.761295 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.761424 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.761507 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.761603 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.762395 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.763057 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:35.763378 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.763510 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.763614 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.763719 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.764570 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.765296 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:35.765630 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.765722 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.765788 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.765870 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.765951 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:35.766075 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:35.766352 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.767078 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:35.767456 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.767592 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.767670 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.767776 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.767918 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:35.768037 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.769800 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.771718 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.772510 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:35.772854 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.772966 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.773032 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.773108 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.773174 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:35.773295 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:35.773584 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.774211 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:35.774580 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.774687 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.774787 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.774889 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.775015 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:35.775118 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.777062 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:35.778876 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.779552 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:35.779840 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.779932 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.780000 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.780091 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.780252 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:35.780856 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.781572 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:35.781864 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.781965 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.782032 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.782109 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.782255 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:35.782768 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.783530 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:35.783871 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.783988 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.784057 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.784137 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.784215 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:35.784337 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:35.784570 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.785213 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:35.785533 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.785621 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.785713 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.785793 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.785869 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:35.786004 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:35.786273 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.787019 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:35.787302 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.787406 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.787471 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.787559 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.787643 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:35.787775 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:35.788056 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.788716 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:35.789052 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.837306 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.837423 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.837524 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.837713 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:35.838414 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.839199 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:35.839484 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.839575 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.839647 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.839722 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.839909 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:35.840377 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:35.841192 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:35.841568 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:35.841698 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:35.841804 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:35.841914 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:35.842039 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:35.842212 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:36.212554 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.334004 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:36.334424 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.334535 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.334602 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.334690 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.334813 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:36.334905 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:36.336639 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:36.339934 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.341140 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:36.341479 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.341588 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.341671 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.341757 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.341878 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:36.341998 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:36.343957 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:36.346204 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.346903 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:36.347147 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.347227 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.347286 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.347352 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.347430 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:36.347566 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:36.347795 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.348449 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:36.348702 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.348780 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.348839 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.348907 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.349008 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:36.349086 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:36.350638 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:36.352289 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.352950 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:36.353215 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.353300 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.353367 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.353469 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.354350 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.355024 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:36.355296 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.355381 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.355466 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.355540 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.356332 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:36.356998 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:36.357301 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:36.357437 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:36.357595 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:36.357758 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:36.357919 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:36.358173 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:36.833879 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:36] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.128570 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.128955 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.129074 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.129140 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.129222 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.129340 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.129449 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.131358 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.134446 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.135206 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:37.135513 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.135598 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.135667 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.135742 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.136666 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.137342 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:37.137667 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.137785 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.137865 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.137962 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.138965 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.139778 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:37.140111 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.140221 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.140306 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.140448 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.140554 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:37.140686 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:37.140988 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.141699 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:37.142004 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.142110 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.142176 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.142251 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.142320 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:37.142441 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:37.142711 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.143310 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.143616 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.144368 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.144491 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.144576 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.144687 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.144767 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.146431 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.148059 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.148745 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:37.149034 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.149124 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.149187 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.149260 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.149327 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:37.149459 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:37.149686 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.150319 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.150664 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.150752 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.150819 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.150932 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.151069 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.151179 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.152647 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.154307 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.155015 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:37.155283 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.155417 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.155519 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.155608 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.155690 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:37.155816 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:37.156088 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.156722 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:37.157021 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.157113 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.157181 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.157264 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.157335 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:37.157460 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:37.157728 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.158429 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:37.158717 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.158806 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.158869 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.158942 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.159111 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:37.159639 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.160635 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:37.160896 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.160978 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.161047 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.161123 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.162198 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.162976 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:37.163328 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.163491 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.163926 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.164044 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.164270 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:37.164870 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.165721 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:37.166055 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.166202 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.166292 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.166380 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.166627 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:37.167451 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.168131 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.168456 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.168555 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.168624 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.168700 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.168802 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.168881 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.170315 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.172165 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.172894 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:37.173191 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.173317 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.173423 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.173532 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.173609 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:37.173744 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:37.441952 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.495432 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:37.495826 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.495911 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.495973 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.496049 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.496115 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:37.496234 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:37.496532 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.497380 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.497665 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.497745 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.497806 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.497873 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.497986 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.498075 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.499711 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.501296 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.501975 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:37.502261 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.502367 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.502456 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.502534 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.502632 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:37.502767 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:37.815342 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.872821 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:37.873259 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.873372 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.873457 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.873543 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.874761 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.875503 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:37.875783 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.875911 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.875991 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.876081 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.876266 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:37.876999 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.877704 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.878000 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.878087 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.878149 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.878221 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.878333 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.881499 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.883631 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.885270 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:37.886188 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:37.886511 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:37.886612 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:37.886678 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:37.886768 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:37.886873 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:37.886954 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.888687 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:37.890259 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:41.987196 140717416466304 app.py:67] Request received: /get_datapoint_ids?\n",
            "I0128 22:51:41.987789 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:41] \"\u001b[37mPOST /get_datapoint_ids HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.088166 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:42.092841 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.092965 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.093062 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.093225 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.094715 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.095505 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:42.095809 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.095901 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.095965 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.096040 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.097053 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.110580 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:42.110816 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.110896 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.110970 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.111056 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.111165 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:42.111263 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.112819 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.114360 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.115028 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:42.115269 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.115347 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.115424 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.115495 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.115588 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:42.115716 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:42.116040 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.116934 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:42.174067 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): ['86e8e19d6f7895b81a5ba6f89c30c499']\n",
            "I0128 22:51:42.174189 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 1 misses out of 281 inputs\n",
            "I0128 22:51:42.174252 140717416466304 caching.py:231] Prepared 1 inputs for model\n",
            "I0128 22:51:42.435468 140717416466304 caching.py:233] Received 1 predictions from model\n",
            "I0128 22:51:42.438797 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.439689 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:42.440113 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.440233 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.440306 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.440406 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.440528 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:42.440613 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.442699 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.444552 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.445353 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:42.447184 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): ['86e8e19d6f7895b81a5ba6f89c30c499']\n",
            "I0128 22:51:42.447280 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 1 misses out of 281 inputs\n",
            "I0128 22:51:42.447341 140717416466304 caching.py:231] Prepared 1 inputs for model\n",
            "I0128 22:51:42.690807 140717416466304 caching.py:233] Received 1 predictions from model\n",
            "I0128 22:51:42.693794 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.694651 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:42.695017 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.695112 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.695177 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.695254 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.695327 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:42.695458 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:42.695706 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.696411 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:42.698008 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.698116 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:42.698176 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.698243 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.698306 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:42.698427 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:42.700462 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.701107 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=RegressionScore\n",
            "I0128 22:51:42.702968 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.703067 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:42.703128 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.703197 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.703265 140717416466304 app.py:189] Requested types: ['RegressionScore']\n",
            "I0128 22:51:42.703377 140717416466304 app.py:199] Will return keys: set()\n",
            "I0128 22:51:42.704471 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=RegressionScore HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.705138 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Scalar\n",
            "I0128 22:51:42.706957 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.707055 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:42.707116 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.707186 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.707250 140717416466304 app.py:189] Requested types: ['Scalar']\n",
            "I0128 22:51:42.707367 140717416466304 app.py:199] Will return keys: set()\n",
            "I0128 22:51:42.708283 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Scalar HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.708928 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:42.710603 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.710700 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:42.710761 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.710834 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.710906 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:42.711016 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:42.712867 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.713526 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=RegressionScore\n",
            "I0128 22:51:42.715317 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.715430 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:42.715507 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.715575 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.715641 140717416466304 app.py:189] Requested types: ['RegressionScore']\n",
            "I0128 22:51:42.715750 140717416466304 app.py:199] Will return keys: set()\n",
            "I0128 22:51:42.716653 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=RegressionScore HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.717328 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Scalar\n",
            "I0128 22:51:42.719035 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.719143 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:42.719225 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.719313 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.719400 140717416466304 app.py:189] Requested types: ['Scalar']\n",
            "I0128 22:51:42.719580 140717416466304 app.py:199] Will return keys: set()\n",
            "I0128 22:51:42.720444 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Scalar HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.721214 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:42.721575 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.721686 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.721771 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.721858 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.722720 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.723419 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:42.723775 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.723870 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.723943 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.724023 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.725007 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.725807 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:42.726182 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.726284 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.726357 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.726457 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.726537 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:42.726651 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:42.726868 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.727570 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:42.727887 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.727983 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.728085 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.728180 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.728304 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:42.728456 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.730085 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.731888 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.732669 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:42.732984 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.733078 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.733150 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.733230 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.733349 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:42.733448 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.734852 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:42.736618 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.737488 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:42.737797 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.737883 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.737957 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.738037 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.738105 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:42.738215 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:42.738471 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:42] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:42.739079 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:42.739372 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:42.739482 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:42.739547 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:42.739622 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:42.739722 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:42.739856 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:43.051181 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:43] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:43.108711 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=umap\n",
            "I0128 22:51:43.111091 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:43.111219 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:51:43.111293 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:43.111403 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:43.111503 140717416466304 projection.py:207] Projection request: instance key: frozenset({('dataset_name', 'rte_dev'), ('proj_kw', frozenset({('n_components', 3)})), ('field_name', 'cls_emb'), ('model_name', 'bert-base-uncased-RTE')})\n",
            "I0128 22:51:43.112271 140717416466304 caching.py:224] CachingModelWrapper 'frozenset({('dataset_name', 'rte_dev'), ('proj_kw', frozenset({('n_components', 3)})), ('field_name', 'cls_emb'), ('model_name', 'bert-base-uncased-RTE')})': misses (dataset=): ['86e8e19d6f7895b81a5ba6f89c30c499']\n",
            "I0128 22:51:43.112364 140717416466304 caching.py:226] CachingModelWrapper 'frozenset({('dataset_name', 'rte_dev'), ('proj_kw', frozenset({('n_components', 3)})), ('field_name', 'cls_emb'), ('model_name', 'bert-base-uncased-RTE')})': 1 misses out of 281 inputs\n",
            "I0128 22:51:43.112449 140717416466304 caching.py:231] Prepared 1 inputs for model\n",
            "inside function\n",
            "   (0, 4)\t0.2912733\n",
            "  (0, 27)\t0.4756689\n",
            "  (0, 33)\t0.27126825\n",
            "  (0, 75)\t0.27274323\n",
            "  (0, 87)\t0.26661512\n",
            "  (0, 106)\t0.28218678\n",
            "  (0, 153)\t0.27540818\n",
            "  (0, 172)\t0.27162588\n",
            "  (0, 180)\t0.2682915\n",
            "  (0, 225)\t0.29036933\n",
            "  (0, 238)\t0.2962363\n",
            "  (0, 258)\t0.29541945\n",
            "  (0, 259)\t0.28324145\n",
            "  (0, 260)\t0.278153\n",
            "  (0, 264)\t0.26406798\n",
            "I0128 22:51:45.589576 140717416466304 caching.py:233] Received 1 predictions from model\n",
            "I0128 22:51:45.591419 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=umap HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.592353 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:45.592784 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.592896 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.592965 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.593050 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.593186 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:45.593275 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.594954 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.596557 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.597240 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.597522 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.597612 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.597674 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.597745 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.597819 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.597950 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.598158 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.598751 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.598991 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.599067 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.599124 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.599190 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.599253 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.599364 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.599590 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.600200 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.600459 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.600540 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.600606 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.600678 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.600746 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.600864 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.601069 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.601718 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:45.601949 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.602026 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.602092 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.602165 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.602344 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:45.602831 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.603465 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:45.603702 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.603785 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.603842 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.603909 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.604061 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:45.604531 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.605209 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:45.605522 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.605614 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.605691 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.605795 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.605976 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:45.606545 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.607254 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:45.607559 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.607639 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.607703 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.607780 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.607850 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:45.607982 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:45.828343 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.947634 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:45.948055 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.948161 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.948237 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.948351 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.948659 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:45.950061 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.950997 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:45.951338 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.951462 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.951539 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.951628 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.951766 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:45.951884 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.953988 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.955745 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.956412 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:45.956716 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.956821 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.956882 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.956959 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.957896 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.958566 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:45.958863 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.958957 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.959027 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.959103 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.959941 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.960629 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:45.960922 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.961009 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.961075 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.961147 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.961936 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.962618 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:45.962960 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.963060 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.963125 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.963198 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.963996 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.964667 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.965037 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.965140 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.965207 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.965291 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.965369 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.965495 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.965709 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.966340 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:45.966656 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.966753 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.966813 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.966887 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.966989 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:45.967079 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.968658 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.970411 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.971060 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:45.971325 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.971425 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.971488 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.971556 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.971665 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:45.971751 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.973339 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:45.975606 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.976323 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.976640 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.976743 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.976830 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.976930 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.977015 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.977143 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.977455 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.978087 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.978410 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.978522 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.978601 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.978681 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.978755 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.978895 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.979147 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.979877 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.980148 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.980244 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.980323 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.980413 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.980486 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.980598 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.980854 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.981607 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:45.981889 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.981984 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.982056 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.982131 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.982203 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:45.982321 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:45.982580 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.983244 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:45.983581 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.983685 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.983755 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.983831 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.983982 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:45.984552 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.985238 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:45.985558 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.985875 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.985976 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.986190 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.986638 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:45.987224 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:45.987951 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:45.988245 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:45.988358 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:45.988442 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:45.988516 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:45.988589 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:45.988730 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:46.371901 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:46] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:46.564094 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:46.564508 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:46.564621 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:46.564691 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:46.564777 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:46.564977 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:46.565618 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:46.566398 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:46.566661 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:46.566740 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:46.566799 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:46.566868 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:46.567014 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:46.567533 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:46.568161 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:46.568480 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:46.568566 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:46.568633 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:46.568707 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:46.568815 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:46.568900 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:46.570706 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:46.572291 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:46] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:46.572941 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:46.573215 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:46.573308 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:46.573378 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:46.573471 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:46.573542 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:46.573679 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:47.021157 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:47.259261 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:47.259808 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:47.259928 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:47.259989 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:47.260073 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:47.261335 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:47.262090 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:47.262369 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:47.262471 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:47.262542 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:47.262614 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:47.262682 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:47.263537 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:47.263885 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:47.264500 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:47.265351 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:47.265462 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:47.265533 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:47.265609 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:47.265719 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:47.265802 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:47.267351 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:47.268938 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:47.270015 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:47.270263 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:47.270345 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:47.270449 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:47.270581 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:47.271632 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:47.272324 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:47.272584 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:47.272666 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:47.272726 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:47.272845 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:47.272970 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:47.273070 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:47.274576 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:47.276082 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:47.276753 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:47.276996 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:47.277076 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:47.277134 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:47.277201 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:47.277297 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:47.277374 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:47.278711 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:47.280422 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:47] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.376028 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:52.376524 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.376641 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.376708 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.376793 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.378036 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.398145 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:52.398462 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.398563 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.398632 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.398714 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.398785 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:52.398903 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:52.399183 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.399803 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:52.400078 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.400173 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.400239 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.400314 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.401344 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.412284 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:52.412554 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.412646 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.412724 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.412806 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.412930 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:52.413055 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:52.414849 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:52.416455 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.418022 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:52.418248 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.418328 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.418402 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.418484 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.418554 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:52.418669 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:52.418883 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.419508 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:52.419737 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.419815 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.419883 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.419959 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.420035 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:52.420149 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:52.420357 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.420983 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:52.421215 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.421301 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.421360 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.421452 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.421552 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:52.421630 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:52.422964 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:52.424553 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.479140 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:52.484686 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.484796 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.484869 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.484947 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.485031 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:52.485146 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:52.485367 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.489020 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:52.489244 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.489326 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.489405 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.489492 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.489562 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:52.489682 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:52.489893 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.490542 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:52.490773 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.490883 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.490955 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.491032 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.491103 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:52.491248 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:52.728491 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.823176 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:52.823678 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.823818 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.823928 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.824084 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.824376 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:52.825483 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.826353 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:52.826685 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.826793 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.826878 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.826986 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.827197 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:52.827735 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.828414 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:52.828666 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.828748 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.828808 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.828876 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.829036 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:52.829559 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.830205 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:52.830516 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.830605 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.830671 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.830746 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.830902 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:52.831351 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.832017 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:52.832334 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.832442 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.832513 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.832591 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.832700 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:52.832785 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:52.834374 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:52.835995 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.836652 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:52.836935 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.837031 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.837099 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.837174 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.837961 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.838667 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:52.838961 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.839079 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.839165 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.839262 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.840405 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:52] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:52.841055 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:52.841336 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:52.841439 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:52.841505 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:52.841578 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:52.841648 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:52.841799 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:53.151301 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:53] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:53.229701 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:53.230239 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:53.230470 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:53.230598 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:53.230750 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:53.230986 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:53.231176 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:53.233840 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:53.235667 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:53] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:53.236463 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:53.236729 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:53.236810 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:53.236869 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:53.236940 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:53.237050 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:53.237129 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:53.238459 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:53.239981 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:53] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:53.240659 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:53.240981 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:53.241083 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:53.241150 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:53.241226 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:53.241338 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:53.241440 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:53.243049 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:53.244559 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:53] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:53.245226 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:53.245503 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:53.245588 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:53.245658 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:53.245732 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:53.245804 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:53.245920 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:53.246139 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:53] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.832839 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:57.833262 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.833373 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.833470 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.833551 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.834566 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.837675 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:57.837988 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.838093 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.838158 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.838234 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.839286 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.891690 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:57.897651 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.897749 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.897821 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.897899 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.897974 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:57.898098 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:57.898334 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.899706 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:57.899946 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.900034 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.900095 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.900166 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.900269 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:57.900347 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.902439 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.904135 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.904817 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:57.905084 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.905182 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.905241 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.905309 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.905427 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:57.905522 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.906858 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.908417 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.914353 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:57.914643 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.914725 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.914785 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.914853 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.914917 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:57.915040 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:57.915256 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.915917 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:57.916164 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.916245 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.916314 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.916378 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.916462 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:57.916573 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:57.916780 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.917434 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:57.917674 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.917752 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.917810 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.917877 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.917941 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:57.918059 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:57.918271 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.918894 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:57.919141 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.919222 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.919281 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.919346 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.919425 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:57.919539 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:57.919750 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.922430 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:57.922680 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.922760 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.922819 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.922911 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.923133 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:57.923863 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.924539 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:57.924835 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.924943 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.925038 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.925133 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.925330 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:57.926079 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.926822 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:51:57.927145 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.927255 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.927337 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.927447 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.927661 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:57.928119 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.928782 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:57.929018 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.929097 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.929156 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.929225 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.929322 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:57.929413 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.930952 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.932646 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.933292 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:51:57.933555 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.933639 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.933698 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.933766 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.933916 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:51:57.934347 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.935009 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:57.935268 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.935349 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.935419 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.935491 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.936174 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.936841 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:57.937147 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.937265 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.937351 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.937465 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.937613 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:57.937733 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.939500 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:57.941086 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:57.941786 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:57.942061 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:57.942149 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:57.942222 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:57.942300 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:57.942372 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:57.942543 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:58.181719 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:58] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:58.278524 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:51:58.279124 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:58.279242 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:58.279316 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:58.279417 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:58.279497 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:51:58.279642 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:51:58.586695 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:58] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:58.707765 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:51:58.708134 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:58.708238 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:58.708331 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:58.708443 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:58.708533 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:51:58.708654 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:51:58.708935 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:58] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:58.709672 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:51:58.709926 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:58.710023 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:58.710085 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:58.710155 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:58.711125 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:58] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:58.711880 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:58.712182 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:58.712276 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:58.712341 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:58.712444 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:58.712556 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:58.712647 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:58.714801 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:58.716829 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:58] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:51:58.717534 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:51:58.717847 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:51:58.717959 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:51:58.718053 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:51:58.718145 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:51:58.718269 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:51:58.718380 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:58.720686 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:51:58.722374 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:51:58] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.593948 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:52:09.594330 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.594459 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.594540 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.594624 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.595618 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.616293 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:52:09.616577 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.616662 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.616723 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.616794 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.617590 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.618281 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:52:09.618560 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.618686 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.618768 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.618849 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.618922 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:52:09.619042 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:52:09.619304 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.676117 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:52:09.676356 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.676466 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.676536 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.676611 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.676721 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:52:09.676803 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:09.678422 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:09.680050 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.680741 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:52:09.680984 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.681063 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.681123 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.681191 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.681291 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:52:09.681366 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:09.682703 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:09.684184 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.684856 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:52:09.685101 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.685181 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.685238 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.685304 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.685365 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:52:09.685508 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:52:09.685725 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.686326 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:52:09.686574 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.686653 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.686710 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.686773 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.686834 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:52:09.686940 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:52:09.687144 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.687781 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:52:09.688016 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.688095 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.688153 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.688218 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.688280 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:52:09.688406 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:52:09.688635 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.689237 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:52:09.695000 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.695095 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.695157 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.695231 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.695406 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:52:09.695906 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.696397 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:52:09.696662 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.696741 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.696806 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.696882 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.696955 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:52:09.697066 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:52:09.697276 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:09.703832 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:52:09.704055 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:09.704135 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:09.704196 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:09.704265 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:09.704333 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:52:09.704490 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:52:09.972507 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:09] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.092909 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:52:10.093419 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.093561 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.093640 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.093750 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.093998 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:52:10.094636 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.095400 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:52:10.095713 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.095817 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.095902 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.095994 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.096216 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:52:10.096950 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.097629 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:52:10.098203 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.098301 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.098367 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.098460 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.098530 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:52:10.098662 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:52:10.360996 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.409961 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input\n",
            "I0128 22:52:10.410332 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.410461 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.410522 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.410603 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.410786 140717416466304 gradient_maps.py:136] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:52:10.411402 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_dot_input HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.412189 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:52:10.412537 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.412641 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.412709 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.412789 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.413896 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.414572 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:52:10.414885 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.414991 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.415071 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.415159 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.415300 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:52:10.415429 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.417821 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.419894 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.420555 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:52:10.420856 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.420946 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.421012 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.421087 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.421820 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.422482 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:52:10.422786 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.422890 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.422971 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.423063 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.423182 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:52:10.423294 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.424636 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.426137 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.426799 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:52:10.427122 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.427212 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.427282 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.427358 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.427478 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:52:10.427556 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.428854 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.430530 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.431160 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:52:10.431479 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.431567 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.431637 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.431712 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.431816 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:52:10.431897 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.433595 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:52:10.435106 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:52:10.435766 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:52:10.436060 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:52:10.436148 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:52:10.436215 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:52:10.436297 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:52:10.436369 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:52:10.436493 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:52:10.436703 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:52:10] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.131877 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:16.132361 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.132500 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.132569 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.132666 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.133936 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.137652 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:16.137905 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.137988 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.138052 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.138129 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.138897 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.139594 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:16.139869 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.139963 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.140029 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.140101 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.140170 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:16.140283 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:16.140573 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.208098 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:16.208335 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.208440 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.208512 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.208582 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.208685 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:16.209933 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.211683 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.213315 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.213980 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:16.214233 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.214313 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.214372 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.214480 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.214587 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:16.214666 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.215953 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.217455 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.218105 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:16.218354 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.218454 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.218528 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.218601 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.218670 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:16.218780 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:16.218999 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.219640 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:16.231770 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.231870 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.231939 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.232036 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.232144 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:16.232231 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.234040 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.235656 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.236145 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:16.236376 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.236485 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.236548 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.236621 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.236691 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:16.236806 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:16.237025 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.237521 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:53:16.237744 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.237823 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.237887 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.237962 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.238039 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:53:16.238171 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:53:16.468511 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.596576 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:53:16.597020 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.597142 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.597213 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.597301 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.597510 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:53:16.598098 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.598894 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:16.599206 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.599308 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.599374 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.599471 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.599586 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:16.599688 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.601309 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.602925 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.603620 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:53:16.603891 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.603981 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.604043 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.604127 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.604240 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:53:16.604395 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:53:16.926259 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.955600 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:16.955976 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.956096 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.956160 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.956248 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.956325 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:16.956497 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:16.956842 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.957613 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:16.957941 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.958032 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.958100 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.958177 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.959168 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.959853 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:16.960145 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.960239 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.960306 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.960403 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.960496 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:16.960615 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:16.960904 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.961533 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:16.961847 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.961952 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.962018 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.962091 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.962209 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:16.962310 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.964447 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.966769 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.967429 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:16.967821 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.967947 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.968018 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.968094 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.968165 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:16.968276 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:16.968595 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.969316 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:53:16.969707 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.969857 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.969977 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.970072 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.970315 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:53:16.971055 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.971749 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:16.972058 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.972162 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.972228 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.972307 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.973139 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:16.973876 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:16.974157 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:16.974294 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:16.974428 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:16.974550 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:16.974686 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:16.974796 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.976556 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:16.978698 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:16] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.305602 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:18.307395 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.307523 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.307587 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.307671 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.308651 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.312948 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:18.313215 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.313296 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.313356 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.313448 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.313517 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:18.313638 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:18.313881 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.327649 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:18.327911 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.328006 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.328073 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.328152 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.328932 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.373458 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:18.378070 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.378170 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.378240 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.378316 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.378442 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:18.378528 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:18.381161 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:18.383145 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.383817 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:18.384076 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.384157 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.384216 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.384283 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.384346 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:18.384470 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:18.384687 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.385334 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:18.385605 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.385687 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.385745 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.385813 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.385910 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:18.385994 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:18.387344 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:18.388866 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.389553 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:53:18.389804 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.389883 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.389943 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.390017 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.390162 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:53:18.390681 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.391316 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:18.391608 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.391716 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.391799 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.391893 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.391996 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:18.392137 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:18.392423 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.393063 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:18.393341 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.393446 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.393512 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.393585 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.393653 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:18.393763 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:18.394020 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.394672 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:18.394987 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.395099 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.395186 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.395282 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.395376 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:18.395534 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:18.395835 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.406268 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:53:18.406689 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.406836 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.406933 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.407046 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.407131 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:53:18.407288 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:53:18.638147 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.712684 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:53:18.713206 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.713352 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.713455 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.713571 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.713814 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:53:18.714487 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:18] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:18.715245 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:53:18.715551 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:18.715635 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:18.715695 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:18.715764 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:18.715830 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:53:18.715967 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:53:19.009014 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.035129 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:19.035576 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.035694 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.035761 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.035851 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.035998 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:19.036094 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.038878 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.040997 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.041748 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:19.042047 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.042133 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.042199 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.042274 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.043091 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.043737 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:19.044037 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.044124 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.044192 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.044269 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.044377 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:19.044483 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.045816 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.047301 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.047951 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:19.048239 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.048329 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.048408 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.048488 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.048594 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:19.048672 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.049963 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.051485 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.052148 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:53:19.052469 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.052564 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.052628 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.052704 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.053432 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.054196 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:53:19.054533 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.054649 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.054731 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.054822 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.054947 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:53:19.055058 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.056717 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:53:19.058419 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:53:19.059062 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:53:19.059298 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:53:19.059377 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:53:19.059484 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:53:19.059572 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:53:19.059661 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:53:19.059805 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:53:19.060055 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:53:19] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.023775 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:25.024842 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.024996 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.025111 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.025221 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.026813 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.045909 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:25.046329 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.046460 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.046526 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.046611 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.047440 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.048133 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:25.048373 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.048480 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.048546 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.048623 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.048695 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:25.048816 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:25.049062 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.110002 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:25.110289 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.110400 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.110480 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.110557 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.110632 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:25.110748 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:25.111011 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.112756 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:25.113009 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.113091 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.113151 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.113217 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.113318 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:25.113412 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.115079 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.116690 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.117350 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:25.117631 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.117713 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.117770 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.117837 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.117932 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:25.118007 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.119302 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.120813 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.121467 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:25.121705 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.121783 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.121842 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.121908 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.121969 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:25.122088 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:25.122295 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.122948 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:25.123182 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.123258 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.123317 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.123382 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.123471 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:25.123604 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:25.359271 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.457480 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:25.457935 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.458096 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.458179 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.458290 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.458420 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:25.458600 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:25.778120 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.855536 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:25.855905 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.855999 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.856065 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.856145 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.856213 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:25.856336 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:25.856640 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.857666 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:25.857914 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.857994 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.858056 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.858130 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.858299 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:25.858866 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.859546 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:25.859848 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.859940 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.860006 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.860083 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.860152 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:25.860264 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:25.860572 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.861266 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:25.861573 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.861685 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.861760 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.861848 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.862049 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:25.862653 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.863288 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:25.863559 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.863649 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.863709 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.863777 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.864602 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.865334 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:25.865681 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.865813 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.865902 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.865998 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.866136 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:25.866228 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.868475 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.870282 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.870945 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:25.871243 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.871331 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.871436 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.871523 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.871631 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:25.871708 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.873306 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.875052 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.875732 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:25.876084 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.876199 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.876280 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.876374 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.877259 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.878080 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:25.878417 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.878557 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.878643 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.878759 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.878850 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:25.878978 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:25.879269 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.879964 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:25.880297 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.880444 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.880551 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.880683 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.880810 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:25.880927 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.882351 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.884077 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:25.884857 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:25.885251 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:25.885360 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:25.885455 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:25.885596 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:25.885782 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:25.885908 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.887452 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:25.889245 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:25] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.475825 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:33.479355 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.479511 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.479589 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.479683 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.481402 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.482236 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:33.482540 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.482622 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.482682 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.482750 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.483597 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.484297 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:33.484556 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.484637 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.484696 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.484763 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.484830 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:33.484953 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:33.485190 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.548659 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:33.553941 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.554062 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.554908 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.555051 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.555208 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:33.555325 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:33.557009 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:33.558634 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.559267 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:33.559544 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.559626 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.559683 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.559748 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.559842 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:33.559917 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:33.561224 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:33.562741 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.563383 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:33.563665 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.563745 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.563804 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.563871 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.563935 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:33.564049 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:33.564262 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.564892 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:33.565135 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.565215 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.565274 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.565340 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.565418 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:33.565536 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:33.565748 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.572134 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:33.572374 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.572482 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.572555 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.572626 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.572697 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:33.572810 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:33.573038 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.573691 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:33.573925 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.574010 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.574070 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.574162 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.574313 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:33.574805 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:33.575476 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:33.575712 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:33.575791 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:33.575849 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:33.575916 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:33.575987 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:33.576125 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:33.933109 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:33] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.124652 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:34.125798 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.126067 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.126319 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.126480 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.127238 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:34.127465 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:34.127848 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.128687 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:34.129028 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.129139 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.129212 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.129310 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.129551 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:34.130378 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.131078 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:34.131332 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.131428 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.131490 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.131560 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.131628 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:34.131761 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:34.591966 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.984187 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:34.984612 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.984725 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.984791 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.984876 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.985837 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.986664 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:34.986936 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.987020 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.987081 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.987151 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.987217 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:34.987331 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:34.987658 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.988345 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:34.988690 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.988804 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.988882 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.988957 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.989071 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:34.989152 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:34.990795 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:34.993207 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.993902 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:34.994213 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.994318 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.994405 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.994513 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:34.994650 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:34.994753 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:34.996205 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:34.997766 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:34] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:34.998625 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:34.998903 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:34.998984 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:34.999043 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:34.999110 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.000281 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.000968 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:35.001278 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.001398 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.001479 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.001555 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.001662 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:35.001742 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.003147 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.004666 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.005333 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:35.005678 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.005770 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.005838 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.005914 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.006048 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:35.006180 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.007526 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.009060 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.311372 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:35.313223 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.313367 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.313481 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.313573 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.314543 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.315206 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:35.315492 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.315581 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.315651 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.315726 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.316604 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.317286 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:35.319051 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.319208 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.319295 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.319411 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.319674 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:35.319905 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:35.320268 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.390594 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:35.390866 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.390968 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.391037 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.391114 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.391222 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:35.391311 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.393186 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.394915 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.395612 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:35.395870 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.395951 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.396009 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.396079 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.396189 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:35.396286 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.397890 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.399939 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.400632 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:35.403572 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.403695 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.403773 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.403849 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.403923 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:35.404037 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:35.404252 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.404903 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:35.405170 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.405250 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.405309 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.405375 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.405505 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:35.405583 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.407362 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:35.409352 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.410036 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:35.410346 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.410484 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.410562 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.410640 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.410713 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:35.410847 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:35.798494 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.900841 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:35.901202 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.901296 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.901364 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.901465 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.901537 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:35.901663 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:35.901972 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.902731 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:35.903003 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.903084 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.903144 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.903212 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.903369 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:35.903964 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.904618 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:35.904913 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.905021 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.905092 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.905168 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.905323 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:35.905810 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.906466 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:35.906759 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.906857 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.906924 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.907010 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.907082 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:35.907193 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:35.907427 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.908061 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:35.908337 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.908446 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.908514 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.908593 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.908663 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:35.908773 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:35.908981 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:35] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:35.909626 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:35.909867 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:35.909945 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:35.910007 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:35.910073 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:35.910137 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:35.910270 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:36.334526 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:36.561145 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:36.562068 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:36.562241 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:36.562329 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:36.562483 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:36.562669 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:36.562795 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:36.565413 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:36.568173 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:36.569186 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:36.569555 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:36.569676 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:36.569754 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:36.569844 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:36.569969 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:36.570067 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:36.572126 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:36.574697 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:36.575351 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:36.575620 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:36.575703 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:36.575767 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:36.575840 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:36.576878 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:36.577779 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:36.578182 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:36.578346 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:36.578500 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:36.578624 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:36.579922 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:36.582018 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:36.582444 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:36.582589 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:36.582677 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:36.582764 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:36.582843 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:36.583011 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:36.583360 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:36.584090 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:36.584428 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:36.584530 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:36.584603 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:36.584684 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:36.584795 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:36.584883 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:36.586310 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:36.588030 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:36] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.813837 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:37.816632 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.816807 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.816920 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.817040 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.818174 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.818821 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:37.819075 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.819157 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.819235 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.819312 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.819399 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:37.819529 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:37.819785 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.820485 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:37.820741 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.820825 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.820894 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.820967 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.821914 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.881783 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:37.882105 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.882208 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.882275 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.882352 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.882436 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:37.882556 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:37.882774 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.883442 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:37.883695 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.883774 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.883834 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.883900 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.884009 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:37.884102 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:37.885734 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:37.887341 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.888028 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:37.888277 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.888359 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.888442 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.888511 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.888575 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:37.888685 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:37.888895 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.889537 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:37.889782 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.889861 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.889920 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.889993 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.890136 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:37.890673 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.905073 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:37.905292 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.905401 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.905473 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.905546 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.905615 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:37.905727 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:37.905941 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.906629 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:37.906941 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.907068 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.907158 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.907257 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.907417 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:37.907544 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:37.909569 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:37.911784 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:37] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:37.912464 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:37.912711 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:37.912815 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:37.912890 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:37.912977 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:37.913046 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:37.913184 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:38.288772 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:38] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:38.436930 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:38.437362 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:38.437536 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:38.437618 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:38.437733 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:38.437974 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:38.439142 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:38] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:38.439946 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:38.440229 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:38.440313 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:38.440373 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:38.440482 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:38.440555 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:38.440694 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:38.889236 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:38] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.212080 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:39.212479 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.212604 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.212669 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.212758 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.212844 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:39.212992 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:39.213284 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.214031 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:39.214302 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.214398 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.214497 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.214570 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.214688 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:39.214777 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.216460 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.218104 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.218782 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:39.219098 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.219189 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.219257 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.219345 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.220134 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.220801 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:39.221059 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.221139 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.221199 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.221266 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.221366 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:39.221472 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.222782 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.224410 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.225084 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:39.225332 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.225428 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.225491 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.225560 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.225624 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:39.225737 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:39.225952 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.226618 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:39.226859 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.226938 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.227005 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.227075 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.227919 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.228596 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:39.228900 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.229016 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.229084 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.229162 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.229269 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:39.229349 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.230836 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.232827 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.233522 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:39.233833 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.233922 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:39.233996 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.234081 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.234209 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:39.234317 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships., including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.236235 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:39.237912 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:39.665561 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:39.752712 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:39.752865 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 281 inputs\n",
            "I0128 22:54:39.752930 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:39.753027 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:39.753105 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:39.753233 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:39.755552 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:39] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.588366 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:40.588803 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.588900 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.588961 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.589041 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.590008 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.594440 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:40.594691 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.594777 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.594836 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.594908 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.595705 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.596430 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:40.596693 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.596776 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.596836 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.596904 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.596975 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:40.597090 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:40.597328 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.634248 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:40.639410 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.639511 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.639583 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.639667 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.639780 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:40.639864 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:40.642399 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:40.644647 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.645377 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:40.645711 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.645824 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.645908 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.646004 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.646084 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:40.646198 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:40.646453 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.647115 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:40.647373 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.647479 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.647545 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.647622 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.647729 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:40.647809 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:40.649156 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:40.650685 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.651353 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:40.651621 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.651703 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.651762 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.651829 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.651893 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:40.652009 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:40.652273 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.652911 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:40.653224 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.653339 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.653448 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.653551 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.653641 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:40.653759 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:40.653983 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.654623 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:40.654859 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.654940 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.655005 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.655075 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.655223 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:40.655733 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.656380 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:40.656719 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.656838 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.656929 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.657041 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.657210 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:40.657737 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.658372 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:40.658629 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.658706 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.658764 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.658831 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.658894 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:40.659008 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:40.659222 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:40] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:40.659837 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:40.660078 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:40.660165 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:40.660249 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:40.660344 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:40.660460 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:40.660632 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:41.004682 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.155227 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:41.155785 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.155925 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.156002 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.156090 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.156219 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:41.156375 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.158746 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.160501 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.161273 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:41.161586 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.161673 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.161742 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.161821 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.161892 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:41.162039 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:41.595665 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.917619 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:41.918070 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.918228 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.918313 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.918468 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.919633 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.920400 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:41.920677 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.920758 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.920817 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.920888 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.920992 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:41.921072 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.922725 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.924343 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.925025 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:41.925285 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.925368 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.925469 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.925542 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.925613 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:41.925727 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:41.925950 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.926628 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:41.926880 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.926978 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.927042 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.927117 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.927973 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.928644 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:41.928904 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.928983 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.929042 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.929108 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.929202 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:41.929277 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.930663 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.932172 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:41.932847 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:41.933133 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:41.933223 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:41.933292 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:41.933369 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:41.933498 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:41.933576 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.935081 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:41.936720 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:41] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.131205 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:44.131693 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.131808 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.131880 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.131971 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.133220 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.153509 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:44.153799 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.153882 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.153942 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.154012 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.154809 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.155474 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:44.155711 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.155789 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.155850 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.155919 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.155986 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:44.156103 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:44.156366 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.188045 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:44.193522 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.193629 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.193696 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.193772 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.193884 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:44.193983 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.196656 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.198261 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.198964 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:44.199221 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.199301 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.199359 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.199445 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.199545 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:44.199622 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.200953 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.202500 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.203207 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:44.203535 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.203652 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.203733 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.203825 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.203916 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:44.204066 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:44.204365 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.205062 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:44.205367 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.205508 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.205600 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.205694 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.205784 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:44.205902 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:44.206126 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.206746 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:44.207001 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.207082 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.207140 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.207207 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.207269 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:44.207378 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:44.207613 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.208238 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:44.208506 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.208590 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.208660 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.208740 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.208891 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:44.209331 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.209984 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:44.210291 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.210379 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.210468 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.210540 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.210691 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:44.211155 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.211803 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:44.212056 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.212136 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.212196 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.212263 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.212327 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:44.212478 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:44.490233 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.586534 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:44.586899 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.586995 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.587069 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.587157 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.587232 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:44.587362 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:44.587661 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.588525 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:44.588779 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.588861 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.588918 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.588987 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.589054 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:44.589191 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:44.857481 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.963249 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:44.963860 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.963979 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.964052 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.964145 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.964275 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:44.964374 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.966894 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.969369 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.970220 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:44.970554 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.970669 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.970755 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.970847 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.970975 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:44.971123 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.972828 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.974594 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.975277 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:44.975622 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.975731 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.975800 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.975914 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.976989 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.977989 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:44.978292 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.978417 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.978510 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.978608 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.978696 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:44.978823 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:44.979076 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.979804 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:44.980122 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.980270 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.980416 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.980537 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.981543 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.982267 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:44.982622 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.982727 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.982822 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.982901 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.983073 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:44.983178 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.984942 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.987551 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:44.988228 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:44.988561 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:44.988671 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:44.988746 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:44.988836 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:44.988972 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:44.989085 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.990651 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:44.992691 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:44] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:45.991144 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:45.991627 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:45.991751 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:45.991816 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:45.991898 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:45.993013 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:45] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:45.996427 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:45.996692 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:45.996774 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:45.996834 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:45.996902 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:45.996980 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:45.997096 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:45.997346 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:45] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:45.998015 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:45.998261 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:45.998345 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:45.998422 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:45.998496 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:45.999259 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:45] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.053908 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:46.054156 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.054239 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.054299 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.054368 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.054497 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:46.054580 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.056188 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.057808 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.058498 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:46.058748 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.058828 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.058887 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.058955 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.059025 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:46.059138 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:46.059350 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.059998 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:46.082375 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.082514 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.082586 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.082664 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.082736 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:46.082852 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:46.083075 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.083726 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:46.084050 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.084161 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.084237 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.084419 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.084567 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:46.084706 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:46.085007 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.085697 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:46.086008 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.086122 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.086209 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.086304 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.086416 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:46.086574 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:46.086877 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.087563 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:46.087875 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.087992 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.088080 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.088179 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.088313 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:46.088450 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.090587 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.092650 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.093300 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:46.093569 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.093651 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.093710 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.093778 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.093921 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:46.094416 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.095066 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:46.095343 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.095441 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.095514 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.095589 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.095660 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:46.095811 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:46.325814 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.401301 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:46.401681 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.401776 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.401838 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.401916 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.402052 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:46.402162 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.403792 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.405461 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.406193 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:46.406487 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.406572 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.406637 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.406713 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.407548 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.408198 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:54:46.408483 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.408567 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.408632 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.408706 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.408777 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:54:46.408894 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:54:46.409116 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.409766 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:54:46.410044 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.410131 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.410200 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.410269 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.411042 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.411713 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:46.412014 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.412102 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.412174 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.412250 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.412358 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:46.412458 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.413768 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.415264 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.415928 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:54:46.416224 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.416313 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.416380 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.416472 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.416625 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:54:46.417068 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.417734 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:54:46.418040 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.418130 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.418200 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.418275 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.418346 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:54:46.418492 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:54:46.750638 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.827661 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:46.828163 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.828298 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.828371 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.828482 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.828621 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:46.828742 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.831123 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.832746 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:54:46.833480 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:54:46.833761 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:54:46.833845 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:54:46.833905 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:54:46.833975 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:54:46.834079 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:54:46.834156 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.835488 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:54:46.836982 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:54:46] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.528140 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:27.528898 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.529023 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.529108 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.529218 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.530588 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.531598 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:27.531840 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.531920 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.532001 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.532075 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.532886 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.550047 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:27.550332 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.550439 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.550515 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.550587 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.550656 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:27.550772 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:27.551034 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.563719 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:27.563960 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.564042 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.564103 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.564170 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.564275 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:27.564354 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:27.566067 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:27.567683 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.569129 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:27.569364 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.569468 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.569534 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.569608 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.569712 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:27.569806 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:27.571129 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:27.572652 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.602224 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:27.602482 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.602567 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.602631 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.602705 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.602775 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:27.602887 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:27.603106 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.603875 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:27.604129 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.604205 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.604262 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.604327 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.604402 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:27.604519 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:27.604728 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.609374 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:55:27.609631 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.609710 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.609769 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.609842 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.610009 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:55:27.610477 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.611098 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:27.611357 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.611455 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.611525 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.611595 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.611664 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:27.611776 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:27.611999 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.612637 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:55:27.612917 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.613012 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.613084 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.613159 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.613228 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:55:27.613367 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:55:27.843708 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.869823 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:55:27.870198 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.870296 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.870358 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.870475 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.870659 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:55:27.871267 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:27] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:27.872311 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:55:27.872578 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:27.872660 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:27.872721 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:27.872797 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:27.872860 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:55:27.872997 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:55:28.184367 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.242115 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:28.242624 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.242789 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.242900 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.243021 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.243126 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:28.243271 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:28.243725 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.244524 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:28.244882 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.245017 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.245102 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.245190 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.245330 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:28.245519 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.247500 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.249364 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.250041 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:28.250346 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.250463 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.250529 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.250614 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.250727 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:28.250828 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.252247 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.253870 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.254547 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:28.254879 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.255025 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.255104 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.255181 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.256062 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.256789 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:28.257092 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.257177 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.257249 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.257328 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.257428 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:28.257570 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:28.257863 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.258527 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:28.258819 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.258921 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.258997 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.259068 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.259170 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:28.259250 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.260704 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.262376 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.263156 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:28.263535 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.263664 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.263775 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.263889 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.265141 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:28.265836 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:28.266183 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:28.266303 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:28.266400 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:28.266516 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:28.266700 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:28.266844 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi cities, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.268843 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi townships.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:28.271051 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:28] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.484290 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:56.484733 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.484836 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.484903 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.484998 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.485973 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.506901 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:56.507146 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.507227 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.507289 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.507356 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.507441 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:56.507565 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:56.507802 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.508440 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:56.508683 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.508762 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.508820 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.508887 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.509861 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.572752 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:56.577443 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.577544 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.577620 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.577706 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.577841 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:56.577932 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:56.579612 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:56.582038 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.583507 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:55:56.583783 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.583884 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.583956 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.584037 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.584199 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:55:56.584759 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.585460 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:56.585719 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.585803 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.585866 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.585937 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.586041 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:56.586124 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:56.587525 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:56.589107 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.589882 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:56.590111 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.590193 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.590260 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.590334 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.590428 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:56.590552 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:56.590778 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.591439 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:56.591731 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.591826 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.591898 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.591977 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.592054 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:56.592173 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:56.592466 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.593113 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:56.593435 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.593542 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.593640 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.594049 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.594180 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:56.594311 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:56.594671 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:56] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:56.595352 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:55:56.600529 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:56.600624 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:56.600688 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:56.600765 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:56.600836 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:55:56.600985 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:55:57.014523 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.394964 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads\n",
            "I0128 22:55:57.395354 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.395475 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.395554 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.395638 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.395720 140717416466304 app.py:189] Requested types: ['Tokens', 'AttentionHeads']\n",
            "I0128 22:55:57.395865 140717416466304 app.py:199] Will return keys: {'layer_5/attention', 'layer_2/attention', 'layer_11/attention', 'layer_9/attention', 'layer_10/attention', 'layer_0/attention', 'layer_7/attention', 'tokens_sentence1', 'layer_3/attention', 'tokens', 'layer_4/attention', 'tokens_sentence2', 'layer_8/attention', 'layer_1/attention', 'layer_6/attention'}\n",
            "I0128 22:55:57.767078 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=Tokens,AttentionHeads HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.948717 140717416466304 app.py:67] Request received: /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:57.949080 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.949174 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.949239 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.949319 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.949403 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:57.949534 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:57.949839 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_preds?model=albert-base-v2-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.950587 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm\n",
            "I0128 22:55:57.950880 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.950962 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.951022 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.951089 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.951248 140717416466304 gradient_maps.py:67] Found fields for gradient attribution: ['token_grad_sentence1', 'token_grad_sentence2']\n",
            "I0128 22:55:57.951869 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=grad_norm HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.952346 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:57.952607 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.952688 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.952748 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.952816 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.952921 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:57.953011 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.954646 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.956217 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.956738 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:57.956972 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.957052 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.957118 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.957191 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.957297 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:57.957379 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.959407 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.960900 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.961593 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:57.961886 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.961975 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.962042 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.962118 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.962221 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:57.962298 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.963626 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.965125 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.965790 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:57.966033 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.966112 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.966172 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.966239 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.967020 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.967693 140717416466304 app.py:67] Request received: /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics\n",
            "I0128 22:55:57.967935 140717416466304 caching.py:224] CachingModelWrapper 'albert-base-v2-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.968014 140717416466304 caching.py:226] CachingModelWrapper 'albert-base-v2-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.968072 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.968139 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.968996 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=albert-base-v2-RTE&dataset_name=rte_dev&interpreter=metrics HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.969668 140717416466304 app.py:67] Request received: /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds\n",
            "I0128 22:55:57.969953 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.970050 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.970124 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.970200 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.970286 140717416466304 app.py:189] Requested types: ['MulticlassPreds']\n",
            "I0128 22:55:57.970418 140717416466304 app.py:199] Will return keys: {'probas'}\n",
            "I0128 22:55:57.970689 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_preds?model=bert-base-uncased-RTE&dataset_name=rte_dev&requested_types=MulticlassPreds HTTP/1.1\u001b[0m\" 200 -\n",
            "I0128 22:55:57.971330 140717416466304 app.py:67] Request received: /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer\n",
            "I0128 22:55:57.971626 140717416466304 caching.py:224] CachingModelWrapper 'bert-base-uncased-RTE': misses (dataset=rte_dev): []\n",
            "I0128 22:55:57.971712 140717416466304 caching.py:226] CachingModelWrapper 'bert-base-uncased-RTE': 0 misses out of 1 inputs\n",
            "I0128 22:55:57.971772 140717416466304 caching.py:231] Prepared 0 inputs for model\n",
            "I0128 22:55:57.971838 140717416466304 caching.py:233] Received 0 predictions from model\n",
            "I0128 22:55:57.971935 140717416466304 lemon_explainer.py:87] Found text fields for LEMON attribution: ['sentence1', 'sentence2']\n",
            "I0128 22:55:57.972012 140717416466304 lemon_explainer.py:108] Explaining: U.S. forces have been engaged in intense fighting after insurgents launched simultaneous attacks in several Iraqi Fallujah and Baqubah are Iraqi townships, including Fallujah and Baqubah.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.973375 140717416466304 lemon_explainer.py:108] Explaining: Fallujah and Baqubah are Iraqi cities.\n",
            "The exact solution is  x = 0                              \n",
            "I0128 22:55:57.974926 140717416466304 _internal.py:113] 127.0.0.1 - - [28/Jan/2021 22:55:57] \"\u001b[37mPOST /get_interpretations?model=bert-base-uncased-RTE&dataset_name=rte_dev&interpreter=counterfactual%20explainer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfsAqAKfbWsi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}